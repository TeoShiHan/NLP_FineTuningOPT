{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJqElfMrd3s"
      },
      "source": [
        " **Few shot text generation with T5 Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7DFNSwyfFuu"
      },
      "source": [
        "Author: Ramsri Goutham Golla\n",
        "\n",
        "Linkedin : https://www.linkedin.com/in/ramsrig/\n",
        "\n",
        "Twitter: https://twitter.com/ramsri_goutham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4D0OZzQaB3I"
      },
      "source": [
        "## 1. Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhRIV6wXSFbH",
        "outputId": "09adcafe-d991-432d-d492-0f932dcf6a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 13.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 16.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=1e291769aaa0e2e968d0a2f18fbce53315bd66703bb52cd0a997ac58101bc121\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==2.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojc3j-S6VS_t",
        "outputId": "45b109c0-cef9-4bc6-888f-2497a059c72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Mar 29 04:02:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check we have a GPU and check the memory size of the GUP\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hpcn7EOWHII"
      },
      "source": [
        "## 2. Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f6neDi6_VxU7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    OPTForCausalLM, AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "a1d68d6d05e34ec18635852c458bc88a",
            "653a649b4d7044db9ddb2c8ca764f312",
            "ea068002da5e473f838bd3905fa30b78",
            "4bac5261dc4d4948825c8805c2a12d9d",
            "cb08690858184a3ba93a180f9dd81d32",
            "c211d9752fde4264bc8ebe73421f2e59",
            "5696b50e0e414a478f8bd8614cdd78b9",
            "591b94e723d64fbc8fa08b063814ef79",
            "5a10449bfed74c9b8785373a30938d7c",
            "475840e8ece44597bfd15674861782d5",
            "b6117c50a2be4836ac762652c6867fd7",
            "30df01e3e1da485a9ec1dd16034415fd",
            "22c0bcfd22c74c2dafe57241603e2e1b",
            "321cc867c13a45538637b1dedee0fe76",
            "b3a6f09f140843cbaa7f244803e27f66",
            "05b2efa9cb2849429e83c0a619827d2d",
            "3676b292e42a4305b5fb7f5185fe99c3",
            "af91fe433ab14b62aa387c1b7c528e28",
            "1eee06d8d6024e8397870365777030d7",
            "ca5acaa5fc6843e0a6d0aecb28ff3a13",
            "81ae8842ae0b40149431999fc8c84426",
            "e0dd01c250fb49a6bf77fc5f321c9777",
            "42023657f5a7462aa69371e7cda93dea",
            "9b4a1b91eaf14854a3577301d9a4388b"
          ]
        },
        "id": "CTzBRXGNvaF9",
        "outputId": "294989ab-a207-41ff-c78c-bd01a2bbcef0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "opt = OPTForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
              "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8KlFP1jpzizh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# optimizer\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in opt.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in opt.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "82WbxhWMFKWe"
      },
      "outputs": [],
      "source": [
        "# dataset preparation\n",
        "\n",
        "true_false_adjective_tuples = [\n",
        "                               (\"The cat is alive\",\"The cat is dead\"),\n",
        "                               (\"The old woman is beautiful\",\"The old woman is ugly\"),\n",
        "                               (\"The purse is cheap\",\"The purse is expensive\"),\n",
        "                               (\"Her hair is curly\",\"Her hair is straight\"),\n",
        "                               (\"The bathroom is clean\",\"The bathroom is dirty\"),\n",
        "                               (\"The exam was easy\",\"The exam was difficult\"),\n",
        "                               (\"The house is big\",\"The house is small\"),\n",
        "                               (\"The house owner is good\",\"The house owner is bad\"),\n",
        "                               (\"The little kid is fat\",\"The little kid is thin\"),\n",
        "                               (\"She arrived early\",\"She arrived late.\"),\n",
        "                               (\"John is very hardworking\",\"John is very lazy\"),\n",
        "                               (\"The fridge is empty\",\"The fridge is full\")\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkjy-dsENoc"
      },
      "source": [
        "## 3. Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQHqVp9UGzro",
        "outputId": "e1cca6fd-c77e-45ae-e9d7-d96585332838"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  1\n",
            "epoch  2\n",
            "epoch  3\n",
            "epoch  4\n",
            "epoch  5\n",
            "epoch  6\n",
            "epoch  7\n",
            "epoch  8\n",
            "epoch  9\n"
          ]
        }
      ],
      "source": [
        "opt.train()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print (\"epoch \",epoch)\n",
        "  for input,output in true_false_adjective_tuples:\n",
        "    input_sent = \"falsify: \"+input+ \" </s>\"\n",
        "    ouput_sent = output+\" </s>\"\n",
        "\n",
        "    tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "    tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "    input_ids  = tokenized_inp[\"input_ids\"]\n",
        "    attention_mask = tokenized_inp[\"attention_mask\"]\n",
        "\n",
        "    lm_labels= tokenized_output[\"input_ids\"]\n",
        "    decoder_attention_mask=  tokenized_output[\"attention_mask\"]\n",
        "\n",
        "\n",
        "    # the forward function automatically creates the correct decoder_input_ids\n",
        "    output = opt(input_ids=input_ids, labels=lm_labels,attention_mask=attention_mask)\n",
        "    loss = output[0]\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnSgB7e5pdp"
      },
      "source": [
        "## 4. Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpUH01FvMuus",
        "outputId": "b33571b7-ccb5-43cc-bacf-cef25aca2e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "falsify: The sailor was happy and joyful. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\743309940.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\743309940.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3468</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3465 │   │   # Convert inputs to python lists</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3466 │   │   </span>token_ids = to_py_obj(token_ids)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3467 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3468 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469 │   │   │   </span>token_ids=token_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3470 │   │   │   </span>skip_special_tokens=skip_special_tokens,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3471 │   │   │   </span>clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">949</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_decode</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">946 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">947 │   │   │   │   </span>current_sub_text.append(token)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">948 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> current_sub_text:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>949 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>sub_texts.append(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_tokens_to_string(current_sub_text))              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">950 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">951 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> spaces_between_special_tokens:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">952 │   │   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>.join(sub_texts)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">316</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, tokens):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>.join(tokens)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 │   │   </span>text = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bytearray</span>([<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.byte_decoder[c] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> c <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> text]).decode(<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>, errors=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> text                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>sequence item <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>: expected str instance, NoneType found\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\743309940.py\u001b[0m:\u001b[94m18\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\743309940.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m3468\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mdecode\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3465 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Convert inputs to python lists\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3466 \u001b[0m\u001b[2m│   │   \u001b[0mtoken_ids = to_py_obj(token_ids)                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3467 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3468 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._decode(                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3469 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_ids=token_ids,                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3470 \u001b[0m\u001b[2m│   │   │   \u001b[0mskip_special_tokens=skip_special_tokens,                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3471 \u001b[0m\u001b[2m│   │   │   \u001b[0mclean_up_tokenization_spaces=clean_up_tokenization_spaces,                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m949\u001b[0m in     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_decode\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m946 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m947 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcurrent_sub_text.append(token)                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m948 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m current_sub_text:                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m949 \u001b[2m│   │   │   \u001b[0msub_texts.append(\u001b[96mself\u001b[0m.convert_tokens_to_string(current_sub_text))              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m950 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m951 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m spaces_between_special_tokens:                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m952 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m.join(sub_texts)                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m :\u001b[94m316\u001b[0m in \u001b[92mconvert_tokens_to_string\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mconvert_tokens_to_string\u001b[0m(\u001b[96mself\u001b[0m, tokens):                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m316 \u001b[2m│   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m.join(tokens)                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0mtext = \u001b[96mbytearray\u001b[0m([\u001b[96mself\u001b[0m.byte_decoder[c] \u001b[94mfor\u001b[0m c \u001b[95min\u001b[0m text]).decode(\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m, errors=\u001b[96mse\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m text                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0msequence item \u001b[1;36m30\u001b[0m: expected str instance, NoneType found\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_sent = 'falsify: The sailor was happy and joyful. </s>'\n",
        "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
        "\n",
        "test_input_ids  = test_tokenized[\"input_ids\"]\n",
        "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
        "\n",
        "opt.eval()\n",
        "beam_outputs = opt.generate(\n",
        "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
        "    max_length=64,\n",
        "    early_stopping=True,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print (sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea-svE2qjgSH",
        "outputId": "d3e45f98-890e-473e-943a-05fe562e112e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "falsify: This is a safe neighbourhood. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\2293813077.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\2293813077.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3468</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3465 │   │   # Convert inputs to python lists</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3466 │   │   </span>token_ids = to_py_obj(token_ids)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3467 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3468 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469 │   │   │   </span>token_ids=token_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3470 │   │   │   </span>skip_special_tokens=skip_special_tokens,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3471 │   │   │   </span>clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">949</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_decode</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">946 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">947 │   │   │   │   </span>current_sub_text.append(token)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">948 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> current_sub_text:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>949 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>sub_texts.append(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_tokens_to_string(current_sub_text))              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">950 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">951 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> spaces_between_special_tokens:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">952 │   │   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>.join(sub_texts)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">316</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, tokens):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>.join(tokens)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 │   │   </span>text = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bytearray</span>([<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.byte_decoder[c] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> c <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> text]).decode(<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>, errors=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> text                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>sequence item <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>: expected str instance, NoneType found\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\2293813077.py\u001b[0m:\u001b[94m18\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\2293813077.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m3468\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mdecode\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3465 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Convert inputs to python lists\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3466 \u001b[0m\u001b[2m│   │   \u001b[0mtoken_ids = to_py_obj(token_ids)                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3467 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3468 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._decode(                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3469 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_ids=token_ids,                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3470 \u001b[0m\u001b[2m│   │   │   \u001b[0mskip_special_tokens=skip_special_tokens,                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3471 \u001b[0m\u001b[2m│   │   │   \u001b[0mclean_up_tokenization_spaces=clean_up_tokenization_spaces,                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m949\u001b[0m in     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_decode\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m946 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m947 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcurrent_sub_text.append(token)                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m948 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m current_sub_text:                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m949 \u001b[2m│   │   │   \u001b[0msub_texts.append(\u001b[96mself\u001b[0m.convert_tokens_to_string(current_sub_text))              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m950 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m951 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m spaces_between_special_tokens:                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m952 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m.join(sub_texts)                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m :\u001b[94m316\u001b[0m in \u001b[92mconvert_tokens_to_string\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mconvert_tokens_to_string\u001b[0m(\u001b[96mself\u001b[0m, tokens):                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m316 \u001b[2m│   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m.join(tokens)                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0mtext = \u001b[96mbytearray\u001b[0m([\u001b[96mself\u001b[0m.byte_decoder[c] \u001b[94mfor\u001b[0m c \u001b[95min\u001b[0m text]).decode(\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m, errors=\u001b[96mse\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m text                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0msequence item \u001b[1;36m30\u001b[0m: expected str instance, NoneType found\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_sent = 'falsify: This is a safe neighbourhood. </s>'\n",
        "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
        "\n",
        "test_input_ids  = test_tokenized[\"input_ids\"]\n",
        "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
        "\n",
        "opt.eval()\n",
        "beam_outputs = opt.generate(\n",
        "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
        "    max_length=64,\n",
        "    early_stopping=True,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print (sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxyTXk_Dj7R0",
        "outputId": "1cdeb44e-14e0-4685-9dcc-3eb19b30aab9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\4128596348.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\4128596348.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3468</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3465 │   │   # Convert inputs to python lists</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3466 │   │   </span>token_ids = to_py_obj(token_ids)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3467 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3468 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469 │   │   │   </span>token_ids=token_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3470 │   │   │   </span>skip_special_tokens=skip_special_tokens,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3471 │   │   │   </span>clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">949</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_decode</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">946 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">947 │   │   │   │   </span>current_sub_text.append(token)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">948 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> current_sub_text:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>949 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>sub_texts.append(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_tokens_to_string(current_sub_text))              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">950 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">951 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> spaces_between_special_tokens:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">952 │   │   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>.join(sub_texts)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">316</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_tokens_to_string</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, tokens):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>.join(tokens)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 │   │   </span>text = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bytearray</span>([<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.byte_decoder[c] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> c <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> text]).decode(<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>, errors=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> text                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>sequence item <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>: expected str instance, NoneType found\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_17520\\4128596348.py\u001b[0m:\u001b[94m18\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17520\\\\4128596348.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m3468\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mdecode\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3465 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Convert inputs to python lists\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3466 \u001b[0m\u001b[2m│   │   \u001b[0mtoken_ids = to_py_obj(token_ids)                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3467 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3468 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._decode(                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3469 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_ids=token_ids,                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3470 \u001b[0m\u001b[2m│   │   │   \u001b[0mskip_special_tokens=skip_special_tokens,                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3471 \u001b[0m\u001b[2m│   │   │   \u001b[0mclean_up_tokenization_spaces=clean_up_tokenization_spaces,                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m:\u001b[94m949\u001b[0m in     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_decode\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m946 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m947 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcurrent_sub_text.append(token)                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m948 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m current_sub_text:                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m949 \u001b[2m│   │   │   \u001b[0msub_texts.append(\u001b[96mself\u001b[0m.convert_tokens_to_string(current_sub_text))              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m950 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m951 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m spaces_between_special_tokens:                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m952 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m.join(sub_texts)                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m :\u001b[94m316\u001b[0m in \u001b[92mconvert_tokens_to_string\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mconvert_tokens_to_string\u001b[0m(\u001b[96mself\u001b[0m, tokens):                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m316 \u001b[2m│   │   \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m.join(tokens)                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0mtext = \u001b[96mbytearray\u001b[0m([\u001b[96mself\u001b[0m.byte_decoder[c] \u001b[94mfor\u001b[0m c \u001b[95min\u001b[0m text]).decode(\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m, errors=\u001b[96mse\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m text                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0msequence item \u001b[1;36m31\u001b[0m: expected str instance, NoneType found\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_sent = 'falsify: The tortoise was very slow. </s>'\n",
        "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
        "\n",
        "test_input_ids  = test_tokenized[\"input_ids\"]\n",
        "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
        "\n",
        "opt.eval()\n",
        "beam_outputs = opt.generate(\n",
        "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
        "    max_length=64,\n",
        "    early_stopping=True,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=2\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print (sent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "02cc78f39b956e7a9e388ae47d6d57214dc392b813a5a0ac9421587740002e11"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05b2efa9cb2849429e83c0a619827d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eee06d8d6024e8397870365777030d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0dd01c250fb49a6bf77fc5f321c9777",
            "max": 891691430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ae8842ae0b40149431999fc8c84426",
            "value": 891691430
          }
        },
        "22c0bcfd22c74c2dafe57241603e2e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "30df01e3e1da485a9ec1dd16034415fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b2efa9cb2849429e83c0a619827d2d",
            "placeholder": "​",
            "style": "IPY_MODEL_b3a6f09f140843cbaa7f244803e27f66",
            "value": " 1.20k/1.20k [00:32&lt;00:00, 36.4B/s]"
          }
        },
        "321cc867c13a45538637b1dedee0fe76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3676b292e42a4305b5fb7f5185fe99c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1eee06d8d6024e8397870365777030d7",
              "IPY_MODEL_ca5acaa5fc6843e0a6d0aecb28ff3a13"
            ],
            "layout": "IPY_MODEL_af91fe433ab14b62aa387c1b7c528e28"
          }
        },
        "42023657f5a7462aa69371e7cda93dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "475840e8ece44597bfd15674861782d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bac5261dc4d4948825c8805c2a12d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591b94e723d64fbc8fa08b063814ef79",
            "placeholder": "​",
            "style": "IPY_MODEL_5696b50e0e414a478f8bd8614cdd78b9",
            "value": " 792k/792k [00:34&lt;00:00, 23.1kB/s]"
          }
        },
        "5696b50e0e414a478f8bd8614cdd78b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "591b94e723d64fbc8fa08b063814ef79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a10449bfed74c9b8785373a30938d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6117c50a2be4836ac762652c6867fd7",
              "IPY_MODEL_30df01e3e1da485a9ec1dd16034415fd"
            ],
            "layout": "IPY_MODEL_475840e8ece44597bfd15674861782d5"
          }
        },
        "653a649b4d7044db9ddb2c8ca764f312": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ae8842ae0b40149431999fc8c84426": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9b4a1b91eaf14854a3577301d9a4388b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d68d6d05e34ec18635852c458bc88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea068002da5e473f838bd3905fa30b78",
              "IPY_MODEL_4bac5261dc4d4948825c8805c2a12d9d"
            ],
            "layout": "IPY_MODEL_653a649b4d7044db9ddb2c8ca764f312"
          }
        },
        "af91fe433ab14b62aa387c1b7c528e28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a6f09f140843cbaa7f244803e27f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6117c50a2be4836ac762652c6867fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_321cc867c13a45538637b1dedee0fe76",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22c0bcfd22c74c2dafe57241603e2e1b",
            "value": 1199
          }
        },
        "c211d9752fde4264bc8ebe73421f2e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5acaa5fc6843e0a6d0aecb28ff3a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b4a1b91eaf14854a3577301d9a4388b",
            "placeholder": "​",
            "style": "IPY_MODEL_42023657f5a7462aa69371e7cda93dea",
            "value": " 892M/892M [00:31&lt;00:00, 28.0MB/s]"
          }
        },
        "cb08690858184a3ba93a180f9dd81d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e0dd01c250fb49a6bf77fc5f321c9777": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea068002da5e473f838bd3905fa30b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c211d9752fde4264bc8ebe73421f2e59",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb08690858184a3ba93a180f9dd81d32",
            "value": 791656
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
