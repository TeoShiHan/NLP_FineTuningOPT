{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ACQUISITION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PARANMT_50M_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://paperswithcode.com/dataset/paranmt-50m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-12387731b9302b2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>where a reference is made to this paragraph, article 5 of regulation ( eu ) no 182/2011 shall apply.</s>where reference is made to this article, article 5 of regulation ( eu ) no 182 / 2011 shall apply.\n",
      "</s>don't worry, pete, i'm coming to save you.</s>don't worry, pete.\n",
      "</s>opening of sitting the sitting opened at 09.00. 2.</s>start of the session the session started at 9 p.m.\n",
      "</s>it'il get as fat as you are.</s>he 'll be as fat as you.\n",
      "</s>that had struck jess and will as funny, because everyone knew that your posterior was the scientific name for your situpon.</s>this seemed to be jess and will's funny, since everyone knew that the back tie was a scientific name for the butt.\n",
      "</s>his long, lethal fingers rhythmically clawed the ground as they gained strength.</s>the long, deadly fingers moved rhythmically into the soil, and the original forces were slow.\n",
      "</s>oh... honey.</s>oh, baby...\n",
      "</s>i couldn't tell half the time if he was talking... or you were reading his mind.</s>i couldn't tell if he was talking or reading his thoughts.\n",
      "</s>yeah, how are you doing, huh?</s>how's it going, huh?\n",
      "</s>a bigger, fatter man got out behind him.</s>behind him stood a larger, stronger man.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# current working directory changes when imported from other modules, so to ensure para_nmt_path is correct we store\n",
    "# the absolute path to the module for reference.\n",
    "package_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "\n",
    "class PARANMT_50M_DATASET(LightningDataModule):\n",
    "    \n",
    "    file_path = os.path.join(package_directory, \"para-nmt-5m-processed.zip\")\n",
    "\n",
    "    def __init__(self, opt_type, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_type = opt_type\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_type)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_type, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            batch = examples['text']\n",
    "            processed_batch = list()\n",
    "            for i in batch:\n",
    "                # replace the \\t splitting with a '</s>' token to denote source-target\n",
    "                processed_batch.append(str.replace(i, \"\\t\", self.tokenizer.eos_token))\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"text\", data_files=self.file_path, streaming=True)['train']\n",
    "        \n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        \n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
    "        \n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = PARANMT_50M_DATASET(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PARABANK_Dataset\n",
    "https://paperswithcode.com/dataset/parabank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>E-3612/10 (RO) Elena Oana Antonescu (PPE) to the Commission (25 May 2010)</s>\n",
      "</s>Much of the poor majority of the world is mired in a vicious circle of disease, poverty, and political instability.</s>A great deal of the poor majority of the world is drowned in an enchanted ring of ills, poverty, and political instalency.\n",
      "</s>You promised me 24 hours.</s>I've been promised 24 hours.\n",
      "</s>Management of deep-sea fish stocks (vote)</s>Management of fishing stocks in deep waters (vote)\n",
      "</s>Forty seconds!</s>40 seconds!\n",
      "</s>Regulation (EEC) No 3846/87 should therefore be amended accordingly.</s>Regulation (EEC) No 3846/87 should therefore be amended in accordance with the above-mentioned provisions.\n",
      "</s>Richard saved me.</s>Richard rescued me.\n",
      "</s>He had a good teacher.</s>He had a good tutor.\n",
      "</s>For the first time since 1974.</s>First since 1974.\n",
      "</s>You are in my hospital.</s>You are in my Hospital.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "README from parabank-2.0.zip\n",
    "The TSV file contains ParaBank 2, a diverse collection of paraphrases generated\n",
    "through bilingual generation. Details of the dataset and how it's created can\n",
    "be found here:\n",
    "Hu, J. E., A. Singh, N. Holzenberger, M. Post, & B. Van Durme. 2019. Large-scale,\n",
    "Diverse, Paraphrastic Bitexts via Sampling and Clustering. Proceedings of CoNLL 2019,\n",
    "Hong Kong, Nov 3 – Nov 4, 2019.\n",
    "Each line of the file contains a bilingual dual-condition score, a reference\n",
    "sentence, and paraphrases of the same reference sentence. A reference sentence may\n",
    "have between one to five distinct paraphrases. The lines are in descending\n",
    "order of the dual-conditioned score, a measurement of the quality of the\n",
    "original bilingual sentence pair. Within the same line, paraphrases are ranked by\n",
    "model score as described in the paper - i.e., the first paraphrase from left\n",
    "to right correspond to the system with subscript \"1\" in evaluation, and the\n",
    "last to \"5\". All sentences are raw text (untokenized). The reference sentences\n",
    "appear in ascending order of their bidirectional model scores (the lower the\n",
    "better), which we use to filter the bilingual resource used to generate ParaBank 2.\n",
    "\"\"\"\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class PARABANK_Dataset(LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule for parabank dataset for causal language modelling\n",
    "    Note on num_workers: https://github.com/huggingface/datasets/pull/4375\n",
    "    IterableDatasets do not support Dataloaders with num_workers > 0. Watch the PR to see if the fix will be merged.\n",
    "    \"\"\"\n",
    "    parabank_url = \"http://cs.jhu.edu/~vandurme/data/parabank-2.0.zip\"\n",
    "\n",
    "    def __init__(self, opt_name, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_name = opt_name\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_name)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_name, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            batch = examples['text']\n",
    "            processed_batch = list()\n",
    "            for i in batch:\n",
    "                # split by \\t (it is a tsv file) and omit the initial dual-condition score (it is useless)\n",
    "                i = i.split('\\t')[1:]\n",
    "                # filter entries without paraphrases and split them with a '</s>' token to denote source-target\n",
    "                if len(i) > 1:\n",
    "                    processed_batch.append(i[0] + self.tokenizer.eos_token + i[1])\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"text\", data_files=self.parabank_url, streaming=True)['train']\n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = PARABANK_Dataset(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QUORA_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataCombiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-a233796b5026b737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': ['E-3612/10 (RO) Elena Oana Antonescu (PPE) to the Commission (25 May 2010)</s>']}\n",
      "{'source': ['E-3612/10 (RO) Elena Oana Antonescu (PPE) to the Commission (25 May 2010)</s>']}\n",
      "{'source': ['Much of the poor majority of the world is mired in a vicious circle of disease, poverty, and political instability.</s>A great deal of the poor majority of the world is drowned in an enchanted ring of ills, poverty, and political instalency.']}\n",
      "{'source': [\"You promised me 24 hours.</s>I've been promised 24 hours.\"]}\n",
      "{'source': ['Management of deep-sea fish stocks (vote)</s>Management of fishing stocks in deep waters (vote)']}\n",
      "{'source': ['Much of the poor majority of the world is mired in a vicious circle of disease, poverty, and political instability.</s>A great deal of the poor majority of the world is drowned in an enchanted ring of ills, poverty, and political instalency.']}\n",
      "{'source': [\"You promised me 24 hours.</s>I've been promised 24 hours.\"]}\n",
      "{'source': ['Management of deep-sea fish stocks (vote)</s>Management of fishing stocks in deep waters (vote)']}\n",
      "{'source': ['Forty seconds!</s>40 seconds!']}\n",
      "{'source': ['Regulation (EEC) No 3846/87 should therefore be amended accordingly.</s>Regulation (EEC) No 3846/87 should therefore be amended in accordance with the above-mentioned provisions.']}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Type, Optional\n",
    "\n",
    "from datasets import IterableDataset, interleave_datasets\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class DataCombiner(LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule for combining different datasets for causal language modelling\n",
    "    Note on num_workers: https://github.com/huggingface/datasets/pull/4375\n",
    "    IterableDatasets do not support Dataloaders with num_workers > 0. Watch the PR to see if the fix will be merged.\n",
    "    \"\"\"\n",
    "    def __init__(self, opt_name, batch_size, steps_per_epoch, datamodules: List[Type[LightningDataModule]],\n",
    "                 probabilities: List[float], num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            Name of model type\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        datamodules: List[Type[LightningDataModule]]\n",
    "            List specifying the datamodules whose datasets will be interleaved\n",
    "        probabilities: List[float]\n",
    "            List of probabilities for respective datamodules that should sum to 1\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.opt_name = opt_name\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "        self.datamodules = datamodules\n",
    "        self.probabilities = probabilities\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "        # sanity check\n",
    "        assert sum(self.probabilities) == 1, \"Probabilities for interleaved datasets do not sum to 1.0\"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_name)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # tokenizer is not actually used once instantiated but to stay consistent with other datamodule implementations\n",
    "        # we instantiate it anyway\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_name, use_fast=False)\n",
    "\n",
    "        # instantiate all the datamodules and extract the dataset from them\n",
    "        datasets = list()\n",
    "        for datamodule in self.datamodules:\n",
    "            dm = datamodule(self.opt_name, self.batch_size, self.steps_per_epoch,\n",
    "                            seed=self.seed, pre_tokenize=self.pre_tokenize)\n",
    "            dm.setup()\n",
    "            datasets.append(dm.dataset)\n",
    "\n",
    "        self.dataset = interleave_datasets(datasets, probabilities=self.probabilities, seed=self.seed)\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = DataCombiner(model_name, 1, 1000, [PARABANK_Dataset, PARABANK_Dataset],\n",
    "                                        probabilities=[0.35, 0.65], seed=1337, pre_tokenize=False)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(next(it))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE TUNE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import OPTForCausalLM\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class FineTuneOPT(LightningModule):\n",
    "    \"\"\"\n",
    "    very straightforward direct fine tuning on the OPT model\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"facebook/opt-350m\"):\n",
    "        super().__init__()\n",
    "        self.model = OPTForCausalLM.from_pretrained(model_name)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        # we care only about the last token being predicted\n",
    "        pred_token_logits = logits[:, -1, :]\n",
    "        pred_token = torch.argmax(pred_token_logits, dim=-1)\n",
    "        labels = batch[\"labels\"][:, -1]\n",
    "\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": pred_token, \"labels\": labels}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.model.parameters(), **wandb.config[\"optimizer_params\"])\n",
    "\n",
    "        # configure learning rate scheduler\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, **wandb.config[\"lr_scheduler_params\"])\n",
    "\n",
    "        lr_scheduler_config = {\"scheduler\": lr_scheduler}\n",
    "        lr_scheduler_config.update(wandb.config[\"lr_scheduler_config\"])\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
    "\n",
    "    \"\"\"\n",
    "    Note on following hooks (on_train_epoch_start and on_validation_epoch_start):\n",
    "    Using the following code to access dataloaders: self.train_dataloader().dataset.set_epoch(self.current_epoch) \n",
    "    Results in an exception like such : pytorch_lightning.utilities.exceptions.MisconfigurationException: \n",
    "    `val_dataloader` must be implemented to be used with the Lightning Trainer \n",
    "    Although train_dataloader() is a valid hook, the hook is overridden only in the datamodule and we cannot reference\n",
    "    that. We have to use self.trainer.train_dataloader.dataset which returns some CombinedDataset and then .datasets\n",
    "    that one to get the original TorchIterableDataset.\n",
    "    On the other hand, we can access validation dataloaders with self.trainer.val_dataloaders[0].dataset as that one is\n",
    "    apparently a list and not a CombinedDataset.\n",
    "    Pain.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every train epoch\n",
    "        self.trainer.train_dataloader.dataset.datasets.set_epoch(self.trainer.current_epoch)\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every validation epoch\n",
    "        self.trainer.val_dataloaders[0].dataset.set_epoch(self.trainer.current_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteosh-wp19\u001b[0m (\u001b[33mnlp-assignment\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\HADOOP\\Desktop\\NLP\\wandb\\run-20221226_143347-18ajqu3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-assignment/fine-tune-opt/runs/18ajqu3d\" target=\"_blank\">flowing-leaf-1</a></strong> to <a href=\"https://wandb.ai/nlp-assignment/fine-tune-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:345: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:608: UserWarning: Checkpoint directory C:\\Users\\HADOOP\\Desktop\\NLP\\training_checkpoints\\07-06-2022-optimize exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | OPTForCausalLM | 1.3 B \n",
      "-----------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,263.032 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:152: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:106: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 500/500 [4:23:16<00:00, 31.59s/it, loss=4.89, v_num=qu3d]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>▆▆▃▁▁▅▄▅▅▅▅▄▅█▅▅▄▄▆▄▃▆▃▃▂▃▅▃▃▂▃▃▂▂▄▂▂▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▃▆▇▆█▆▆▃▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_loss</td><td>5.36745</td></tr><tr><td>trainer/global_step</td><td>7499</td></tr><tr><td>val_loss</td><td>4.8551</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flowing-leaf-1</strong>: <a href=\"https://wandb.ai/nlp-assignment/fine-tune-opt/runs/18ajqu3d\" target=\"_blank\">https://wandb.ai/nlp-assignment/fine-tune-opt/runs/18ajqu3d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221226_143347-18ajqu3d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # initialisation steps\n",
    "    torch.cuda.empty_cache()\n",
    "    AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "    run = wandb.init(project=\"fine-tune-opt\")\n",
    "\n",
    "    with run:\n",
    "        datamodule = DataCombiner(wandb.config[\"model_name\"], batch_size=wandb.config[\"batch_size\"],\n",
    "                                            steps_per_epoch=wandb.config[\"steps_per_epoch\"],\n",
    "                                            datamodules=[PARABANK_Dataset, PARANMT_50M_DATASET],\n",
    "                                            probabilities=[0.5, 0.5])\n",
    "        datamodule.setup()\n",
    "\n",
    "        if (wandb.config[\"load_from_checkpoint\"] is not None) and (os.path.isfile(wandb.config[\"load_from_checkpoint\"])):\n",
    "            model = FineTuneOPT.load_from_checkpoint(checkpoint_path=wandb.config[\"load_from_checkpoint\"])\n",
    "        else:\n",
    "            model = FineTuneOPT(wandb.config[\"model_name\"])\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=wandb.config[\"checkpoint_save_dir\"],\n",
    "                                              save_top_k=2, monitor=\"val_loss\",\n",
    "                                              filename=\"fine-tune-opt-epoch={epoch:03d}-val_loss={val_loss:.3f}\")\n",
    "\n",
    "        # create wandb logger (obviously)\n",
    "        wandb_logger = WandbLogger(checkpoint_callback=False)\n",
    "\n",
    "        print(\"TRAINING MODEL\")\n",
    "        trainer = Trainer(max_epochs=wandb.config[\"max_epochs\"], gpus=AVAIL_GPUS,\n",
    "                          check_val_every_n_epoch=wandb.config[\"check_val_every_n_epoch\"],\n",
    "                          callbacks=[checkpoint_callback],\n",
    "                          logger=wandb_logger)\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate paraphrase examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FineTuneOPT.load_from_checkpoint(checkpoint_path=r\"C:\\Users\\HADOOP\\Desktop\\NLP\\training_checkpoints\\07-06-2022-optimize\\fine-tune-opt-epoch=epoch=027-val_loss=val_loss=4.868.ckpt\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "model = model.eval()\n",
    "i = \"I agree with that\"\n",
    "encoded_inputs = tokenizer([i + \"</s> \"], padding=True, return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I agree with that illed's the Bol.......... Century. o....11. Century........lympics. Century..ics.ics...lympicsope Bol.... Century. Century.. Century Bol...lympics.lympics.lympics.lympics.lympics.lympics.lympics.lympics.lympics.lympics.lymp\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.model.generate(inputs=encoded_inputs['input_ids'].to(\"cuda\"),\n",
    "                                                max_length=100,\n",
    "                                                use_cache=False)\n",
    "\n",
    "outputs = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog is a cute and friendly animal. ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02cc78f39b956e7a9e388ae47d6d57214dc392b813a5a0ac9421587740002e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
