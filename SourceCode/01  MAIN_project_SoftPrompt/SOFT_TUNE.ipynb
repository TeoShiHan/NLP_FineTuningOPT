{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (1.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning==1.6.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (2.11.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (1.24.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (0.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (22.0)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (6.0)\n",
      "Requirement already satisfied: torch>=1.8.* in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (1.13.1+cu117)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pytorch_lightning==1.6.0) (2022.11.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (3.8.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (6.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (1.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (1.51.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (58.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (0.6.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (3.20.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.6.0) (0.38.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.6.0) (1.26.13)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tqdm>=4.41.0->pytorch_lightning==1.6.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch_lightning==1.6.0) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (0.13.7)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (1.12.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: pathtools in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna==2.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (1.24.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (1.9.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (6.7.0)\n",
      "Requirement already satisfied: alembic in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (1.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: cmaes>=0.5.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: cliff in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (22.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (4.64.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from optuna==2.0.0) (1.4.45)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from sqlalchemy>=1.1.0->optuna==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from alembic->optuna==2.0.0) (1.2.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (5.2.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (4.1.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (2.4.2)\n",
      "Requirement already satisfied: PyYAML>=3.12 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (6.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (0.5.1)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cliff->optuna==2.0.0) (3.5.0)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (22.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (1.8.2)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (3.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from importlib-metadata>=4.4->cliff->optuna==2.0.0) (3.11.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from stevedore>=2.0.1->cliff->optuna==2.0.0) (5.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from colorlog->optuna==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from Mako->alembic->optuna==2.0.0) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: pyarrow>=4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (2.28.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (1.5)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (4.2.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (1.24.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: semver in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (2.13.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (4.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: validators>=0.2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (0.20.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (12.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (4.4.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: toml in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (3.1.29)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (9.3.0)\n",
      "Requirement already satisfied: packaging>=14.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (22.0)\n",
      "Requirement already satisfied: watchdog in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from streamlit) (1.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from altair>=3.2.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.4)\n",
      "Requirement already satisfied: toolz in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from click>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from gitpython!=3.1.19->streamlit) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from importlib-metadata>=1.4->streamlit) (3.11.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.19.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas>=0.21.0->streamlit) (2022.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: six>=1.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from python-dateutil->streamlit) (1.16.0)\n",
      "\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.4->streamlit) (2022.12.7)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.4->streamlit) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.4->streamlit) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from requests>=2.4->streamlit) (2.1.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from rich>=10.11.0->streamlit) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from rich>=10.11.0->streamlit) (2.13.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tzlocal>=1.1->streamlit) (2022.7)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from validators>=0.2->streamlit) (5.1.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (3.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (5.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pyngrok) (6.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from pandas) (1.24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadoop\\desktop\\nlp\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install pytorch_lightning==1.6.0\n",
    "%pip install transformers\n",
    "%pip install wandb\n",
    "%pip install pyyaml\n",
    "%pip install optuna==2.0.0\n",
    "%pip install streamlit\n",
    "%pip install protobuf\n",
    "%pip install pyngrok\n",
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 8000'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ACQUISITION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VALIDATION_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b06b12bf792cbc7d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>can it restore heat to a corpse? ''</s>can he return the heat to the corpse? ``\n",
      "</s>`` how about 'yes '? '' onrad asked softly.</s>'how about plain, yes '? 'onrad asked quietly.\n",
      "</s>wide at one end and narrow at the other, its shape reminded harad of the stringed instruments musicians played on feast days.</s>at one end was wide, on the other narrow, and its shape reminded him of a strident instrument that was played by musicians.\n",
      "</s>it involved a squad of imperial junior scouts, a flag ceremony, a wooden footbridge, and my cousin ivan.</s>he was involved in that section of the imperial young scouts, a wooden bridge, and my cousin ivan.\n",
      "</s>lived in a hut, even took vows, or whatever women do in such countries.</s>she lived in a shack, made a promise, or what women do in those countries.\n",
      "</s>the privacy level of the message</s>level of confidential message\n",
      "</s>`` no problem, '' the guard said, heaving the gate open.</s>`` there are no problems, '' he said, opening the gate.\n",
      "</s>i didn't even get a chance to tell herit was running.</s>i didn't even have a chance to tell her it was going to work.\n",
      "</s>approaching from the east.</s>we're coming from the east.\n",
      "</s>crowley gave him the long cool look of someone who has just had a girder dropped in front of his train of thought.</s>crowley gave him the long cold stare of someone who had been thrown by a beam before the train.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# current working directory changes when imported from other modules, so to ensure para_nmt_path is correct we store\n",
    "# the absolute path to the module for reference.\n",
    "package_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "\n",
    "class VALIDATION_DATASET(LightningDataModule):\n",
    "    \n",
    "    file_path = r\"C:\\Users\\HADOOP\\Desktop\\NLP\\validation.txt\"\n",
    "\n",
    "    def __init__(self, opt_type, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_type = opt_type\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_type)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_type, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            batch = examples['text']\n",
    "            processed_batch = list()\n",
    "            for i in batch:\n",
    "                # replace the \\t splitting with a '</s>' token to denote source-target\n",
    "                processed_batch.append(str.replace(i, \"\\t\", self.tokenizer.eos_token))\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"text\", data_files=self.file_path, streaming=True)['train']\n",
    "        \n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        \n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
    "        \n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = VALIDATION_DATASET(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PARANMT_50M_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://paperswithcode.com/dataset/paranmt-50m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-12387731b9302b2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>where a reference is made to this paragraph, article 5 of regulation ( eu ) no 182/2011 shall apply.</s>where reference is made to this article, article 5 of regulation ( eu ) no 182 / 2011 shall apply.\n",
      "</s>don't worry, pete, i'm coming to save you.</s>don't worry, pete.\n",
      "</s>opening of sitting the sitting opened at 09.00. 2.</s>start of the session the session started at 9 p.m.\n",
      "</s>it'il get as fat as you are.</s>he 'll be as fat as you.\n",
      "</s>that had struck jess and will as funny, because everyone knew that your posterior was the scientific name for your situpon.</s>this seemed to be jess and will's funny, since everyone knew that the back tie was a scientific name for the butt.\n",
      "</s>his long, lethal fingers rhythmically clawed the ground as they gained strength.</s>the long, deadly fingers moved rhythmically into the soil, and the original forces were slow.\n",
      "</s>oh... honey.</s>oh, baby...\n",
      "</s>i couldn't tell half the time if he was talking... or you were reading his mind.</s>i couldn't tell if he was talking or reading his thoughts.\n",
      "</s>yeah, how are you doing, huh?</s>how's it going, huh?\n",
      "</s>a bigger, fatter man got out behind him.</s>behind him stood a larger, stronger man.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# current working directory changes when imported from other modules, so to ensure para_nmt_path is correct we store\n",
    "# the absolute path to the module for reference.\n",
    "package_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "\n",
    "class PARANMT_50M_DATASET(LightningDataModule):\n",
    "    \n",
    "    file_path = os.path.join(package_directory, \"para-nmt-5m-processed.zip\")\n",
    "\n",
    "    def __init__(self, opt_type, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_type = opt_type\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_type)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_type, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            batch = examples['text']\n",
    "            processed_batch = list()\n",
    "            for i in batch:\n",
    "                # replace the \\t splitting with a '</s>' token to denote source-target\n",
    "                processed_batch.append(str.replace(i, \"\\t\", self.tokenizer.eos_token))\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"text\", data_files=self.file_path, streaming=True)['train']\n",
    "        \n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        \n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
    "        \n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = PARANMT_50M_DATASET(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PARABANK_Dataset\n",
    "https://paperswithcode.com/dataset/parabank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>E-3612/10 (RO) Elena Oana Antonescu (PPE) to the Commission (25 May 2010)</s>\n",
      "</s>Much of the poor majority of the world is mired in a vicious circle of disease, poverty, and political instability.</s>A great deal of the poor majority of the world is drowned in an enchanted ring of ills, poverty, and political instalency.\n",
      "</s>You promised me 24 hours.</s>I've been promised 24 hours.\n",
      "</s>Management of deep-sea fish stocks (vote)</s>Management of fishing stocks in deep waters (vote)\n",
      "</s>Forty seconds!</s>40 seconds!\n",
      "</s>Regulation (EEC) No 3846/87 should therefore be amended accordingly.</s>Regulation (EEC) No 3846/87 should therefore be amended in accordance with the above-mentioned provisions.\n",
      "</s>Richard saved me.</s>Richard rescued me.\n",
      "</s>He had a good teacher.</s>He had a good tutor.\n",
      "</s>For the first time since 1974.</s>First since 1974.\n",
      "</s>You are in my hospital.</s>You are in my Hospital.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "README from parabank-2.0.zip\n",
    "The TSV file contains ParaBank 2, a diverse collection of paraphrases generated\n",
    "through bilingual generation. Details of the dataset and how it's created can\n",
    "be found here:\n",
    "Hu, J. E., A. Singh, N. Holzenberger, M. Post, & B. Van Durme. 2019. Large-scale,\n",
    "Diverse, Paraphrastic Bitexts via Sampling and Clustering. Proceedings of CoNLL 2019,\n",
    "Hong Kong, Nov 3  Nov 4, 2019.\n",
    "Each line of the file contains a bilingual dual-condition score, a reference\n",
    "sentence, and paraphrases of the same reference sentence. A reference sentence may\n",
    "have between one to five distinct paraphrases. The lines are in descending\n",
    "order of the dual-conditioned score, a measurement of the quality of the\n",
    "original bilingual sentence pair. Within the same line, paraphrases are ranked by\n",
    "model score as described in the paper - i.e., the first paraphrase from left\n",
    "to right correspond to the system with subscript \"1\" in evaluation, and the\n",
    "last to \"5\". All sentences are raw text (untokenized). The reference sentences\n",
    "appear in ascending order of their bidirectional model scores (the lower the\n",
    "better), which we use to filter the bilingual resource used to generate ParaBank 2.\n",
    "\"\"\"\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class PARABANK_Dataset(LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule for parabank dataset for causal language modelling\n",
    "    Note on num_workers: https://github.com/huggingface/datasets/pull/4375\n",
    "    IterableDatasets do not support Dataloaders with num_workers > 0. Watch the PR to see if the fix will be merged.\n",
    "    \"\"\"\n",
    "    parabank_url = \"http://cs.jhu.edu/~vandurme/data/parabank-2.0.zip\"\n",
    "\n",
    "    def __init__(self, opt_name, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_name = opt_name\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_name)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_name, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            batch = examples['text']\n",
    "            processed_batch = list()\n",
    "            for i in batch:\n",
    "                # split by \\t (it is a tsv file) and omit the initial dual-condition score (it is useless)\n",
    "                i = i.split('\\t')[1:]\n",
    "                # filter entries without paraphrases and split them with a '</s>' token to denote source-target\n",
    "                if len(i) > 1:\n",
    "                    processed_batch.append(i[0] + self.tokenizer.eos_token + i[1])\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"text\", data_files=self.parabank_url, streaming=True)['train']\n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = PARABANK_Dataset(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QUORA_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|| 34/34 [00:00<00:00, 68527.79it/s]\n",
      "Resolving data files: 100%|| 2630/2630 [00:00<00:00, 57015.81it/s]\n",
      "Resolving data files: 100%|| 80/80 [00:00<00:00, 161241.86it/s]\n",
      "Using custom data configuration quora.csv-ede1aa58c31497a3\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "Failed to read file 'C:\\Users\\HADOOP\\Desktop\\NLP\\venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\tpu_spawn.py' with error <class 'pandas.errors.ParserError'>: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_15564\\184983851.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">133</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_15564\\\\184983851.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">628</span> in         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 625          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sampler_iter <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 626             # TODO(https://github.com/pytorch/pytorch/issues/76750)</span>                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 627             </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reset()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[call-arg]</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 628 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_data()                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 629          </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._num_yielded += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 630          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_kind == _DatasetKind.Iterable <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 631                </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._IterableDataset_len_called <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">671</span> in         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 668    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 669    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 670       </span>index = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_index()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 671 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_fetcher.fetch(index)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 672       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory:                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 673          </span>data = _utils.pin_memory.pin_memory(data, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory_device)            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 674       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> data                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">34</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fetch</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31          </span>data = []                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> possibly_batched_index:                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>34 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>data.append(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset_iter))                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">StopIteration</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36                </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ended = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37                </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">846</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__iter__</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 843             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._iter_pytorch(worker_info)                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 844             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 845       </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 846 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, example <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._iter():                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 847          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.features:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 848             # `IterableDataset` automatically fills missing columns with None.</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849             # This is done with `_apply_feature_types_on_example`.</span>                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">788</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_iter</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 785          </span>ex_iterable = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ex_iterable.shuffle_data_sources(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._effective_generat  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 786       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 787          </span>ex_iterable = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ex_iterable                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 788 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield from</span> ex_iterable                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 789    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 790    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_iter_shard</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, shard_idx: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 791       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._shuffling:                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">384</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__iter__</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 381       </span>iterator = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">iter</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ex_iterable)                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 382       </span>current_idx = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 383       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.batched:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 384 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, example <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> iterator:                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 385             # If `batched`, first build the batch, if `batch_size` is None or &lt;=0, t</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 386             </span>iterator_batch = (                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 387                </span>iterator                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">577</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__iter__</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 574       </span>indices_iterator = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._iter_random_indices(rng, buffer_size)                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 575       # this is the shuffle buffer that we keep in memory</span>                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 576       </span>mem_buffer = []                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 577 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ex_iterable:                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 578          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(mem_buffer) == buffer_size:  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># if the buffer is full, pick and exampl</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 579             </span>i = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(indices_iterator)                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 580             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield</span> mem_buffer[i]                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">137</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__iter__</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 134       </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Shuffle the kwargs order to shuffle shards\"\"\"</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 135       </span>rng = deepcopy(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generator)                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 136       </span>kwargs_with_shuffled_shards = _shuffle_gen_kwargs(rng, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.kwargs)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 137 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate_examples_fn(**kwargs_with_shuffled_shards)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 138    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 139    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">shard_data_sources</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, shard_idx: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>) -&gt; <span style=\"color: #808000; text-decoration-color: #808000\">\"ExamplesIterable\"</span>:                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 140       </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Keep only the requested shard.\"\"\"</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">713</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 710 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_examples_from_tables_wrapper</span>(generate_tables_fn):                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 711    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>(**kwargs):                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 712       </span>python_formatter = PythonFormatter()                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 713 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, table <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> generate_tables_fn(**kwargs):                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 714          </span>batch = python_formatter.format_batch(table)                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 715          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, example <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(_batch_to_examples(batch)):                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 716             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield</span> <span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>key<span style=\"color: #808000; text-decoration-color: #808000\">}_{</span>i<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>, example                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\packaged_modules\\csv\\csv.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">179</span> in   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_tables</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> file_idx, file <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(itertools.chain.from_iterable(files)):             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177          </span>csv_file_reader = pd.read_csv(file, iterator=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, dtype=dtype, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.confi   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">178          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>179 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> batch_idx, df <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(csv_file_reader):                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180                </span>pa_table = pa.Table.from_pandas(df)                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">181                # Uncomment for debugging (will print the Arrow table size and eleme</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182                # logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1698</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1695    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1696    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; DataFrame:                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1697       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1698 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_chunk()                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1699       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">StopIteration</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1700          </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.close()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1701          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1810</span> in          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_chunk</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1807          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._currow &gt;= <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.nrows:                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1808             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">StopIteration</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1809          </span>size = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">min</span>(size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.nrows - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._currow)                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1810 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.read(nrows=size)                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1811    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1812    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__enter__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; TextFileReader:                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1813       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1778</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1775                </span>index,                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1776                </span>columns,                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1777                </span>col_dict,                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1778 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>) = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.read(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[attr-defined]</span>                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1779                </span>nrows                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1780             </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1781          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">230</span> in  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227       </span>column_names: Sequence[Hashable] | MultiIndex                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">228       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.low_memory:                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>230 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>chunks = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reader.read_low_memory(nrows)                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">231             # destructive to chunks</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">232             </span>data = _concatenate_chunks(chunks)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">233 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">820</span> in                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.parsers.TextReader.read_low_memory</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">866</span> in                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.parsers.TextReader._read_rows</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">852</span> in                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.parsers.TextReader._tokenize_rows</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1973</span> in                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.parsers.raise_parser_error</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ParserError: </span>Error tokenizing data. C error: Expected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> fields in line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, saw <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mC:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_15564\\184983851.py\u001b[0m:\u001b[94m133\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_15564\\\\184983851.py'\u001b[0m                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m:\u001b[94m628\u001b[0m in         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92m__next__\u001b[0m                                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 625 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._sampler_iter \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 626 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 627 \u001b[0m\u001b[2m            \u001b[0m\u001b[96mself\u001b[0m._reset()  \u001b[2m# type: ignore[call-arg]\u001b[0m                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 628 \u001b[2m         \u001b[0mdata = \u001b[96mself\u001b[0m._next_data()                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 629 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m._num_yielded += \u001b[94m1\u001b[0m                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._dataset_kind == _DatasetKind.Iterable \u001b[95mand\u001b[0m \\                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m               \u001b[0m\u001b[96mself\u001b[0m._IterableDataset_len_called \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \\                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m:\u001b[94m671\u001b[0m in         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92m_next_data\u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 668 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 669 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_next_data\u001b[0m(\u001b[96mself\u001b[0m):                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 670 \u001b[0m\u001b[2m      \u001b[0mindex = \u001b[96mself\u001b[0m._next_index()  \u001b[2m# may raise StopIteration\u001b[0m                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 671 \u001b[2m      \u001b[0mdata = \u001b[96mself\u001b[0m._dataset_fetcher.fetch(index)  \u001b[2m# may raise StopIteration\u001b[0m              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 672 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._pin_memory:                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 673 \u001b[0m\u001b[2m         \u001b[0mdata = _utils.pin_memory.pin_memory(data, \u001b[96mself\u001b[0m._pin_memory_device)            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 674 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m data                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m:\u001b[94m34\u001b[0m in \u001b[92mfetch\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m         \u001b[0mdata = []                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m possibly_batched_index:                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mtry\u001b[0m:                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m34 \u001b[2m               \u001b[0mdata.append(\u001b[96mnext\u001b[0m(\u001b[96mself\u001b[0m.dataset_iter))                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mStopIteration\u001b[0m:                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m               \u001b[0m\u001b[96mself\u001b[0m.ended = \u001b[94mTrue\u001b[0m                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mbreak\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m846\u001b[0m in \u001b[92m__iter__\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 843 \u001b[0m\u001b[2m            \u001b[0m\u001b[94myield from\u001b[0m \u001b[96mself\u001b[0m._iter_pytorch(worker_info)                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 844 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 845 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 846 \u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m key, example \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._iter():                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 847 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.features:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 848 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# `IterableDataset` automatically fills missing columns with None.\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# This is done with `_apply_feature_types_on_example`.\u001b[0m                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m788\u001b[0m in \u001b[92m_iter\u001b[0m     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 785 \u001b[0m\u001b[2m         \u001b[0mex_iterable = \u001b[96mself\u001b[0m._ex_iterable.shuffle_data_sources(\u001b[96mself\u001b[0m._effective_generat  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 786 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 787 \u001b[0m\u001b[2m         \u001b[0mex_iterable = \u001b[96mself\u001b[0m._ex_iterable                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 788 \u001b[2m      \u001b[0m\u001b[94myield from\u001b[0m ex_iterable                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 789 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 790 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_iter_shard\u001b[0m(\u001b[96mself\u001b[0m, shard_idx: \u001b[96mint\u001b[0m):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 791 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._shuffling:                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m384\u001b[0m in \u001b[92m__iter__\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 381 \u001b[0m\u001b[2m      \u001b[0miterator = \u001b[96miter\u001b[0m(\u001b[96mself\u001b[0m.ex_iterable)                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 382 \u001b[0m\u001b[2m      \u001b[0mcurrent_idx = \u001b[94m0\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 383 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.batched:                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 384 \u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m key, example \u001b[95min\u001b[0m iterator:                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 385 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# If `batched`, first build the batch, if `batch_size` is None or <=0, t\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 386 \u001b[0m\u001b[2m            \u001b[0miterator_batch = (                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 387 \u001b[0m\u001b[2m               \u001b[0miterator                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m577\u001b[0m in \u001b[92m__iter__\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 574 \u001b[0m\u001b[2m      \u001b[0mindices_iterator = \u001b[96mself\u001b[0m._iter_random_indices(rng, buffer_size)                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 575 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this is the shuffle buffer that we keep in memory\u001b[0m                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 576 \u001b[0m\u001b[2m      \u001b[0mmem_buffer = []                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 577 \u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.ex_iterable:                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 578 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(mem_buffer) == buffer_size:  \u001b[2m# if the buffer is full, pick and exampl\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 579 \u001b[0m\u001b[2m            \u001b[0mi = \u001b[96mnext\u001b[0m(indices_iterator)                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 580 \u001b[0m\u001b[2m            \u001b[0m\u001b[94myield\u001b[0m mem_buffer[i]                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m137\u001b[0m in \u001b[92m__iter__\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 134 \u001b[0m\u001b[2m      \u001b[0m\u001b[33m\"\"\"Shuffle the kwargs order to shuffle shards\"\"\"\u001b[0m                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 135 \u001b[0m\u001b[2m      \u001b[0mrng = deepcopy(\u001b[96mself\u001b[0m.generator)                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 136 \u001b[0m\u001b[2m      \u001b[0mkwargs_with_shuffled_shards = _shuffle_gen_kwargs(rng, \u001b[96mself\u001b[0m.kwargs)               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 137 \u001b[2m      \u001b[0m\u001b[94myield from\u001b[0m \u001b[96mself\u001b[0m.generate_examples_fn(**kwargs_with_shuffled_shards)               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 138 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 139 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mshard_data_sources\u001b[0m(\u001b[96mself\u001b[0m, shard_idx: \u001b[96mint\u001b[0m) -> \u001b[33m\"\u001b[0m\u001b[33mExamplesIterable\u001b[0m\u001b[33m\"\u001b[0m:                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 140 \u001b[0m\u001b[2m      \u001b[0m\u001b[33m\"\"\"Keep only the requested shard.\"\"\"\u001b[0m                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mwrapper\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_generate_examples_from_tables_wrapper\u001b[0m(generate_tables_fn):                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mwrapper\u001b[0m(**kwargs):                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m      \u001b[0mpython_formatter = PythonFormatter()                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 713 \u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m key, table \u001b[95min\u001b[0m generate_tables_fn(**kwargs):                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 714 \u001b[0m\u001b[2m         \u001b[0mbatch = python_formatter.format_batch(table)                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 715 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m i, example \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(_batch_to_examples(batch)):                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m            \u001b[0m\u001b[94myield\u001b[0m \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mkey\u001b[33m}\u001b[0m\u001b[33m_\u001b[0m\u001b[33m{\u001b[0mi\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m, example                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\datasets\\packaged_modules\\csv\\csv.py\u001b[0m:\u001b[94m179\u001b[0m in   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92m_generate_tables\u001b[0m                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m file_idx, file \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(itertools.chain.from_iterable(files)):             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m         \u001b[0mcsv_file_reader = pd.read_csv(file, iterator=\u001b[94mTrue\u001b[0m, dtype=dtype, **\u001b[96mself\u001b[0m.confi   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m179 \u001b[2m            \u001b[0m\u001b[94mfor\u001b[0m batch_idx, df \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(csv_file_reader):                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m               \u001b[0mpa_table = pa.Table.from_pandas(df)                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m               \u001b[0m\u001b[2m# Uncomment for debugging (will print the Arrow table size and eleme\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m               \u001b[0m\u001b[2m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m:\u001b[94m1698\u001b[0m in \u001b[92m__next__\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1695 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1696 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__next__\u001b[0m(\u001b[96mself\u001b[0m) -> DataFrame:                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1697 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1698 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.get_chunk()                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1699 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mStopIteration\u001b[0m:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1700 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m.close()                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1701 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m:\u001b[94m1810\u001b[0m in          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mget_chunk\u001b[0m                                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1807 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._currow >= \u001b[96mself\u001b[0m.nrows:                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1808 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mStopIteration\u001b[0m                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1809 \u001b[0m\u001b[2m         \u001b[0msize = \u001b[96mmin\u001b[0m(size, \u001b[96mself\u001b[0m.nrows - \u001b[96mself\u001b[0m._currow)                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1810 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.read(nrows=size)                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1811 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1812 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__enter__\u001b[0m(\u001b[96mself\u001b[0m) -> TextFileReader:                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1813 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m:\u001b[94m1778\u001b[0m in \u001b[92mread\u001b[0m     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1775 \u001b[0m\u001b[2m               \u001b[0mindex,                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1776 \u001b[0m\u001b[2m               \u001b[0mcolumns,                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1777 \u001b[0m\u001b[2m               \u001b[0mcol_dict,                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1778 \u001b[2m            \u001b[0m) = \u001b[96mself\u001b[0m._engine.read(  \u001b[2m# type: ignore[attr-defined]\u001b[0m                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1779 \u001b[0m\u001b[2m               \u001b[0mnrows                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1780 \u001b[0m\u001b[2m            \u001b[0m)                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1781 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m:\u001b[94m230\u001b[0m in  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mread\u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m      \u001b[0mcolumn_names: Sequence[Hashable] | MultiIndex                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m228 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m229 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.low_memory:                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m230 \u001b[2m            \u001b[0mchunks = \u001b[96mself\u001b[0m._reader.read_low_memory(nrows)                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# destructive to chunks\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m232 \u001b[0m\u001b[2m            \u001b[0mdata = _concatenate_chunks(chunks)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m233 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx\u001b[0m:\u001b[94m820\u001b[0m in                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mpandas._libs.parsers.TextReader.read_low_memory\u001b[0m                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx\u001b[0m:\u001b[94m866\u001b[0m in                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mpandas._libs.parsers.TextReader._read_rows\u001b[0m                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx\u001b[0m:\u001b[94m852\u001b[0m in                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0m                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\HADOOP\\Desktop\\NLP\\pandas\\_libs\\parsers.pyx\u001b[0m:\u001b[94m1973\u001b[0m in                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mpandas._libs.parsers.raise_parser_error\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'c:\\\\Users\\\\HADOOP\\\\Desktop\\\\NLP\\\\pandas\\\\_libs\\\\parsers.pyx'\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mParserError: \u001b[0mError tokenizing data. C error: Expected \u001b[1;36m1\u001b[0m fields in line \u001b[1;36m3\u001b[0m, saw \u001b[1;36m2\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, IterableDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# current working directory changes when imported from other modules, so to ensure para_nmt_path is correct we store\n",
    "# the absolute path to the module for reference.\n",
    "package_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "\n",
    "class QUORA_DATASET(LightningDataModule):\n",
    "    \n",
    "    hugging_dir = \"HanHan055/quora_question_answer_pair\"\n",
    "    def __init__(self, opt_type, batch_size, steps_per_epoch, num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            name of the OPT model type (i.e. facebook/opt-350m)\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.opt_type = opt_type\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "\n",
    "        # init None to make pycharm happy\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_type)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # load tokenizer (should be cached)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_type, use_fast=False)\n",
    "\n",
    "        # preprocess function for the dataset's entries\n",
    "        def preprocess(examples):\n",
    "            # list of len batch\n",
    "            processed_batch = list()\n",
    "            for i,j in zip(examples['source'], examples['target']):\n",
    "                # replace the \\t splitting with a '</s>' token to denote source-target\n",
    "                processed_batch.append(i+ self.tokenizer.eos_token +j)\n",
    "\n",
    "            if self.pre_tokenize:\n",
    "                outputs = self.tokenizer(\n",
    "                    processed_batch,\n",
    "                    truncation=True,\n",
    "                    max_length=69,\n",
    "                )\n",
    "            else:\n",
    "                outputs = {\"source\": processed_batch}\n",
    "            return outputs\n",
    "\n",
    "        # init dataset in streaming mode\n",
    "        self.dataset = load_dataset(\"csv\",\"quora.csv\", streaming=True)['train']\n",
    "        \n",
    "        # elements within buffer size will be shuffled as they are loaded in\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed, buffer_size=10_000)\n",
    "        \n",
    "        # preprocessing will take place while being streamed by dataloader\n",
    "        self.dataset = self.dataset.map(preprocess, batched=True, remove_columns=['source','target'])\n",
    "        \n",
    "        # ensure pytorch tensors are returned\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = QUORA_DATASET(model_name, 1, 1000, seed=1337)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(datamodule.tokenizer.batch_decode(next(it)['input_ids'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataCombiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_21468\\1081730767.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_21468\\\\1081730767.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'PARABANK_Dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mC:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_21468\\1081730767.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92m<module>\u001b[0m                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[3;31m'C:\\\\Users\\\\HADOOP\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_21468\\\\1081730767.py'\u001b[0m                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'PARABANK_Dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List, Type, Optional\n",
    "\n",
    "from datasets import IterableDataset, interleave_datasets\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class DataCombiner(LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule for combining different datasets for causal language modelling\n",
    "    Note on num_workers: https://github.com/huggingface/datasets/pull/4375\n",
    "    IterableDatasets do not support Dataloaders with num_workers > 0. Watch the PR to see if the fix will be merged.\n",
    "    \"\"\"\n",
    "    def __init__(self, opt_name, batch_size, steps_per_epoch, datamodules: List[Type[LightningDataModule]],\n",
    "                 probabilities: List[float], num_workers=0, seed=69, pre_tokenize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        opt_name: str\n",
    "            Name of model type\n",
    "        batch_size: int\n",
    "            batch_size output by dataloader\n",
    "        steps_per_epoch: int\n",
    "            dataset_size = steps_per_epoch * batch_size\n",
    "            Since we do not know the dataset size we simply leave it to the user to determine how many steps per epoch\n",
    "            we should have.\n",
    "        datamodules: List[Type[LightningDataModule]]\n",
    "            List specifying the datamodules whose datasets will be interleaved\n",
    "        probabilities: List[float]\n",
    "            List of probabilities for respective datamodules that should sum to 1\n",
    "        num_workers: int\n",
    "            refer to note above on PR https://github.com/huggingface/datasets/pull/4375\n",
    "        seed: int\n",
    "            haha funny number\n",
    "        pre_tokenize: bool\n",
    "            should we tokenize the texts (if true: dataset will return tokenized ids instead of source text)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.opt_name = opt_name\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.pre_tokenize = pre_tokenize\n",
    "        self.datamodules = datamodules\n",
    "        self.probabilities = probabilities\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "\n",
    "        # sanity check\n",
    "        assert sum(self.probabilities) == 1, \"Probabilities for interleaved datasets do not sum to 1.0\"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # download and cache\n",
    "        GPT2Tokenizer.from_pretrained(self.opt_name)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # tokenizer is not actually used once instantiated but to stay consistent with other datamodule implementations\n",
    "        # we instantiate it anyway\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.opt_name, use_fast=False)\n",
    "\n",
    "        # instantiate all the datamodules and extract the dataset from them\n",
    "        datasets = list()\n",
    "        for datamodule in self.datamodules:\n",
    "            dm = datamodule(self.opt_name, self.batch_size, self.steps_per_epoch,\n",
    "                            seed=self.seed, pre_tokenize=self.pre_tokenize)\n",
    "            dm.setup()\n",
    "            datasets.append(dm.dataset)\n",
    "\n",
    "        self.dataset = interleave_datasets(datasets, probabilities=self.probabilities, seed=self.seed)\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "        # monkeypatch of __len__ function in the dataloader so that the trainer knows how many\n",
    "        # steps there are per epoch. Sure this violates many programming paradigms but it works.\n",
    "        n = self.steps_per_epoch\n",
    "\n",
    "        def __len__(self):\n",
    "            return n\n",
    "\n",
    "        IterableDataset.__len__ = __len__\n",
    "\n",
    "    # dataloaders are basically all the same since we cannot split a streamed dataset\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataloader = DataLoader(self.dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                num_workers=self.num_workers)\n",
    "        if self.pre_tokenize: dataloader.collate_fn = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"facebook/opt-1.3b\"\n",
    "    datamodule = DataCombiner(model_name, 1, 100, [PARABANK_Dataset, PARANMT_50M_DATASET],\n",
    "                                        probabilities=[0.5,0.5], seed=1337, pre_tokenize=False)\n",
    "    datamodule.setup()\n",
    "    dl = datamodule.val_dataloader()\n",
    "    it = iter(dl)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(next(it))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Promp Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SoftEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copied from https://github.com/kipgparker/soft-prompt-tuning/blob/main/soft_embedding.py\n",
    "It's really that simple. huh.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SoftEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 wte: nn.Embedding,\n",
    "                 n_tokens: int = 10,\n",
    "                 random_range: float = 0.5,\n",
    "                 initialize_from_vocab: bool = True):\n",
    "        \"\"\"appends learned embedding to\n",
    "        Args:\n",
    "            wte (nn.Embedding): original transformer word embedding\n",
    "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
    "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
    "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(SoftEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                                  n_tokens,\n",
    "                                                                                  random_range,\n",
    "                                                                                  initialize_from_vocab))\n",
    "\n",
    "    def initialize_embedding(self,\n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10,\n",
    "                             random_range: float = 0.5,\n",
    "                             initialize_from_vocab: bool = True) -> torch.Tensor:\n",
    "        \"\"\"initializes learned embedding\n",
    "        Args:\n",
    "            same as __init__\n",
    "        Returns:\n",
    "            torch.float: initialized using original schemes\n",
    "        \"\"\"\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"run forward pass\n",
    "        Args:\n",
    "            tokens (torch.long): input tokens before encoding\n",
    "        Returns:\n",
    "            torch.float: encoding of text concatenated with learned task specific embedding\n",
    "        \"\"\"\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SoftOPTModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "from typing import Dict\n",
    "\n",
    "from pytorch_lightning import LightningModule, Callback\n",
    "from torch.optim import Adam, SGD, Optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
    "from transformers.models.opt.modeling_opt import *\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "class SoftOPTModelWrapper(OPTForCausalLM):\n",
    "    \"\"\"Wrapper class for OPTForCausalLM to add learnable embedding functionality\n",
    "    Simply initialise it with from_pretrained OPT files and it should work out of the box.\n",
    "    \"\"\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"soft_embedding.wte.weight\", r\"soft_embedding.learned_embedding\",\n",
    "                                       r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # init parameters for embedding\n",
    "        self.n_tokens = wandb.config[\"embedding_n_tokens\"]\n",
    "        self.init_from_vocab = wandb.config[\"init_from_vocab\"]\n",
    "\n",
    "        # initialise the embedding to learn\n",
    "        self.soft_embedding = SoftEmbedding(self.get_input_embeddings(),\n",
    "                                            n_tokens=self.n_tokens,\n",
    "                                            initialize_from_vocab=self.init_from_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"Incredibly scuffed but we have to set the input embeddings to the soft embeddings only AFTER\n",
    "        the pretrained weights have been loaded in. All parameters are the same as a normal from_pretrained() call\n",
    "        \"\"\"\n",
    "\n",
    "        pretrained_model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "        pretrained_model.set_input_embeddings(pretrained_model.soft_embedding)\n",
    "        return pretrained_model\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.LongTensor] = None,\n",
    "                **kwargs):\n",
    "        \"\"\"Shitty forward pass\n",
    "        need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
    "        even though it does not matter what we pad input_ids with, it's just to make HF happy\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        # Note: concatenation of tensors have to happen on the same device\n",
    "        # concat padding representing our learned embedding tokens for batched inputs\n",
    "        # inputs come in as (batch_size, seq_len) and are padded to be (batch_size, n_tokens + seq_len)\n",
    "        input_ids = torch.cat([torch.full((batch_size, self.n_tokens), 50256).to(input_ids.device), input_ids], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [torch.full((batch_size, self.n_tokens), 1).to(attention_mask.device), attention_mask], dim=1)\n",
    "        if labels is not None:\n",
    "            labels = torch.cat([torch.full((batch_size, self.n_tokens), 50256).to(labels.device), labels], dim=1)\n",
    "\n",
    "        return super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "\n",
    "class ParaphraseOPT(LightningModule):\n",
    "    def __init__(self, model_name=\"facebook/opt-350m\", init_optimizer=None, init_lr_scheduler=None):\n",
    "        super().__init__()\n",
    "        self.model = SoftOPTModelWrapper.from_pretrained(model_name)\n",
    "\n",
    "        # these inits should be exclusively used for loading from checkpoints\n",
    "        # see load_from_custom_save for why.\n",
    "        self.init_optimizer = init_optimizer\n",
    "        self.init_lr_scheduler = init_lr_scheduler\n",
    "\n",
    "        self.save_hyperparameters(\"model_name\")\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        # we care only about the last token being predicted\n",
    "        pred_token_logits = logits[:, -1, :]\n",
    "        pred_token = torch.argmax(pred_token_logits, dim=-1)\n",
    "        labels = batch[\"labels\"][:, -1]\n",
    "\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": pred_token, \"labels\": labels}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # thanks stack overflow!\n",
    "        # https://stackoverflow.com/questions/38460918/regex-matching-a-dictionary-efficiently-in-python\n",
    "        # extracting all the layers that are specified by layers_to_optimize using regex for partial matches\n",
    "        regex_matches = [re.compile(\".*\" + pattern + \".*\").match for pattern in wandb.config[\"layers_to_optimize\"]]\n",
    "        layers_to_optimize = [k for k in self.model.state_dict().keys()\n",
    "                              if any(regex_match(k) for regex_match in regex_matches)]\n",
    "\n",
    "        # configure optimizer\n",
    "        optimizers_key = {\"Adam\": Adam, \"SGD\": SGD}\n",
    "        if self.init_optimizer is None:\n",
    "            \"\"\"\n",
    "            thanks forums! https://discuss.pytorch.org/t/how-to-access-to-a-layer-by-module-name/83797/8\n",
    "            We cannot directly pass in the tensor output (value) from state_dict() since that is not the same reference\n",
    "            as the actual layer, hence we instead look for the layer name with the regex matching, then access the \n",
    "            module by name as below.\n",
    "            \"\"\"\n",
    "            def get_module_by_name(module: Union[torch.Tensor, nn.Module],\n",
    "                                   access_string: str):\n",
    "                \"\"\"Retrieve a module nested in another by its access string.\n",
    "                Works even when there is a Sequential in the module.\n",
    "                \"\"\"\n",
    "                names = access_string.split(sep='.')\n",
    "                return reduce(getattr, names, module)\n",
    "\n",
    "            layers = [get_module_by_name(self.model, layer_name) for layer_name in layers_to_optimize]\n",
    "            # pass in the layers into optimizer\n",
    "            optimizer_type = optimizers_key[wandb.config[\"optimizer_type\"]]\n",
    "            optimizer = optimizer_type(layers, **wandb.config[\"optimizer_params\"])\n",
    "        else:\n",
    "            optimizer = self.init_optimizer\n",
    "\n",
    "        # configure learning rate scheduler\n",
    "        lr_scheduler_key = {\"ReduceLROnPlateau\": ReduceLROnPlateau}\n",
    "        if self.init_lr_scheduler is None:\n",
    "            lr_scheduler_type = lr_scheduler_key[wandb.config[\"lr_scheduler_type\"]]\n",
    "            lr_scheduler = lr_scheduler_type(optimizer, **wandb.config[\"lr_scheduler_params\"])\n",
    "        else:\n",
    "            lr_scheduler = self.init_lr_scheduler\n",
    "\n",
    "        lr_scheduler_config = {\"scheduler\": lr_scheduler}\n",
    "        lr_scheduler_config.update(wandb.config[\"lr_scheduler_config\"])\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_custom_save(cls, model_name, path, optimizer: Optimizer = None, lr_scheduler: _LRScheduler = None):\n",
    "        \"\"\"\n",
    "        Custom save function to load from checkpoints created by SpecificLayersCheckpoint callback.\n",
    "        Unfortunately pytorch lightning locks the optimizers in place after instantiation and there is no clean way\n",
    "        to change them afterwards. There are some workarounds but they all suck too:\n",
    "        https://github.com/PyTorchLightning/pytorch-lightning/discussions/9354\n",
    "        https://github.com/PyTorchLightning/pytorch-lightning/discussions/6131\n",
    "        So the current implementation is to throw in the optimizer and lr_scheduler as optional parameters during model\n",
    "        instantiation before then actually updating the model weights.\n",
    "        To try different optimizers and lr_schedulers change configure_optimizers() directly.\n",
    "        \"\"\"\n",
    "        # load the saved checkpoint\n",
    "        state_dict = torch.load(path)\n",
    "\n",
    "        # load optimizer if required\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
    "\n",
    "        # load lr_scheduler if required\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.load_state_dict(state_dict[\"lr_scheduler\"])\n",
    "\n",
    "        # instantiate lightningmodule with pretrained model\n",
    "        model = cls(model_name, optimizer, lr_scheduler)\n",
    "\n",
    "        # load updated state dict into the model (as long as no layers are named optimizer or lr_scheduler)\n",
    "        model.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "    Note on following hooks (on_train_epoch_start and on_validation_epoch_start):\n",
    "    \n",
    "    Using the following code to access dataloaders: self.train_dataloader().dataset.set_epoch(self.current_epoch) \n",
    "    Results in an exception like such : pytorch_lightning.utilities.exceptions.MisconfigurationException: \n",
    "    `val_dataloader` must be implemented to be used with the Lightning Trainer \n",
    "    \n",
    "    Although train_dataloader() is a valid hook, the hook is overridden only in the datamodule and we cannot reference\n",
    "    that. We have to use self.trainer.train_dataloader.dataset which returns some CombinedDataset and then .datasets\n",
    "    that one to get the original TorchIterableDataset.\n",
    "    \n",
    "    On the other hand, we can access validation dataloaders with self.trainer.val_dataloaders[0].dataset as that one is\n",
    "    apparently a list and not a CombinedDataset.\n",
    "    \n",
    "    Pain.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every train epoch\n",
    "        self.trainer.train_dataloader.dataset.datasets.set_epoch(self.trainer.current_epoch)\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every validation epoch\n",
    "        self.trainer.val_dataloaders[0].dataset.set_epoch(self.trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ParaphraseOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseOPT(LightningModule):\n",
    "    def __init__(self, model_name=\"facebook/opt-350m\", init_optimizer=None, init_lr_scheduler=None):\n",
    "        super().__init__()\n",
    "        self.model = SoftOPTModelWrapper.from_pretrained(model_name)\n",
    "\n",
    "        # these inits should be exclusively used for loading from checkpoints\n",
    "        # see load_from_custom_save for why.\n",
    "        self.init_optimizer = init_optimizer\n",
    "        self.init_lr_scheduler = init_lr_scheduler\n",
    "\n",
    "        self.save_hyperparameters(\"model_name\")\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        # we care only about the last token being predicted\n",
    "        pred_token_logits = logits[:, -1, :]\n",
    "        pred_token = torch.argmax(pred_token_logits, dim=-1)\n",
    "        labels = batch[\"labels\"][:, -1]\n",
    "\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": pred_token, \"labels\": labels}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # thanks stack overflow!\n",
    "        # https://stackoverflow.com/questions/38460918/regex-matching-a-dictionary-efficiently-in-python\n",
    "        # extracting all the layers that are specified by layers_to_optimize using regex for partial matches\n",
    "        regex_matches = [re.compile(\".*\" + pattern + \".*\").match for pattern in wandb.config[\"layers_to_optimize\"]]\n",
    "        layers_to_optimize = [k for k in self.model.state_dict().keys()\n",
    "                              if any(regex_match(k) for regex_match in regex_matches)]\n",
    "\n",
    "        # configure optimizer\n",
    "        optimizers_key = {\"Adam\": Adam, \"SGD\": SGD}\n",
    "        if self.init_optimizer is None:\n",
    "            \"\"\"\n",
    "            thanks forums! https://discuss.pytorch.org/t/how-to-access-to-a-layer-by-module-name/83797/8\n",
    "            We cannot directly pass in the tensor output (value) from state_dict() since that is not the same reference\n",
    "            as the actual layer, hence we instead look for the layer name with the regex matching, then access the \n",
    "            module by name as below.\n",
    "            \"\"\"\n",
    "            def get_module_by_name(module: Union[torch.Tensor, nn.Module],\n",
    "                                   access_string: str):\n",
    "                \"\"\"Retrieve a module nested in another by its access string.\n",
    "                Works even when there is a Sequential in the module.\n",
    "                \"\"\"\n",
    "                names = access_string.split(sep='.')\n",
    "                return reduce(getattr, names, module)\n",
    "\n",
    "            layers = [get_module_by_name(self.model, layer_name) for layer_name in layers_to_optimize]\n",
    "            # pass in the layers into optimizer\n",
    "            optimizer_type = optimizers_key[wandb.config[\"optimizer_type\"]]\n",
    "            optimizer = optimizer_type(layers, **wandb.config[\"optimizer_params\"])\n",
    "        else:\n",
    "            optimizer = self.init_optimizer\n",
    "\n",
    "        # configure learning rate scheduler\n",
    "        lr_scheduler_key = {\"ReduceLROnPlateau\": ReduceLROnPlateau}\n",
    "        if self.init_lr_scheduler is None:\n",
    "            lr_scheduler_type = lr_scheduler_key[wandb.config[\"lr_scheduler_type\"]]\n",
    "            lr_scheduler = lr_scheduler_type(optimizer, **wandb.config[\"lr_scheduler_params\"])\n",
    "        else:\n",
    "            lr_scheduler = self.init_lr_scheduler\n",
    "\n",
    "        lr_scheduler_config = {\"scheduler\": lr_scheduler}\n",
    "        lr_scheduler_config.update(wandb.config[\"lr_scheduler_config\"])\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_custom_save(cls, model_name, path, optimizer: Optimizer = None, lr_scheduler: _LRScheduler = None):\n",
    "        \"\"\"\n",
    "        Custom save function to load from checkpoints created by SpecificLayersCheckpoint callback.\n",
    "        Unfortunately pytorch lightning locks the optimizers in place after instantiation and there is no clean way\n",
    "        to change them afterwards. There are some workarounds but they all suck too:\n",
    "        https://github.com/PyTorchLightning/pytorch-lightning/discussions/9354\n",
    "        https://github.com/PyTorchLightning/pytorch-lightning/discussions/6131\n",
    "        So the current implementation is to throw in the optimizer and lr_scheduler as optional parameters during model\n",
    "        instantiation before then actually updating the model weights.\n",
    "        To try different optimizers and lr_schedulers change configure_optimizers() directly.\n",
    "        \"\"\"\n",
    "        # load the saved checkpoint\n",
    "        state_dict = torch.load(path)\n",
    "\n",
    "        # load optimizer if required\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
    "\n",
    "        # load lr_scheduler if required\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.load_state_dict(state_dict[\"lr_scheduler\"])\n",
    "\n",
    "        # instantiate lightningmodule with pretrained model\n",
    "        model = cls(model_name, optimizer, lr_scheduler)\n",
    "\n",
    "        # load updated state dict into the model (as long as no layers are named optimizer or lr_scheduler)\n",
    "        model.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "    Note on following hooks (on_train_epoch_start and on_validation_epoch_start):\n",
    "    \n",
    "    Using the following code to access dataloaders: self.train_dataloader().dataset.set_epoch(self.current_epoch) \n",
    "    Results in an exception like such : pytorch_lightning.utilities.exceptions.MisconfigurationException: \n",
    "    `val_dataloader` must be implemented to be used with the Lightning Trainer \n",
    "    \n",
    "    Although train_dataloader() is a valid hook, the hook is overridden only in the datamodule and we cannot reference\n",
    "    that. We have to use self.trainer.train_dataloader.dataset which returns some CombinedDataset and then .datasets\n",
    "    that one to get the original TorchIterableDataset.\n",
    "    \n",
    "    On the other hand, we can access validation dataloaders with self.trainer.val_dataloaders[0].dataset as that one is\n",
    "    apparently a list and not a CombinedDataset.\n",
    "    \n",
    "    Pain.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every train epoch\n",
    "        self.trainer.train_dataloader.dataset.datasets.set_epoch(self.trainer.current_epoch)\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        # reshuffle the dataset for every validation epoch\n",
    "        self.trainer.val_dataloaders[0].dataset.set_epoch(self.trainer.current_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpecificLayerCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecificLayersCheckpoint(Callback):\n",
    "    \"\"\"\n",
    "    Custom saving of specific layers into a state_dict that can be loaded in using torch.load()\n",
    "    Ideally, we load in the model with from_pretrained, and then use state_dict.update() to update the\n",
    "    weights of the loaded model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monitor: str, dirpath: str, filename: str,\n",
    "                 every_n_epochs: int, layers_to_save: List[str]):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.dirpath = dirpath\n",
    "        self.filename = filename\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        self.layers_to_save = layers_to_save\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # if model should be saved this epoch (+1 since epoch count starts from 0)\n",
    "        if (trainer.current_epoch + 1) % self.every_n_epochs == 0:\n",
    "            # thanks stack overflow!\n",
    "            # https://stackoverflow.com/questions/38460918/regex-matching-a-dictionary-efficiently-in-python\n",
    "            # extracting all the layers that are specified by layers_to_save using regex for partial matches\n",
    "            regex_matches = [re.compile(\".*\" + pattern + \".*\").match for pattern in self.layers_to_save]\n",
    "            save_dict = {k: v for k, v in pl_module.model.state_dict().items()\n",
    "                         if any(regex_match(k) for regex_match in regex_matches)}\n",
    "\n",
    "            # save the optimizer\n",
    "            if pl_module.optimizers() is not None:\n",
    "                save_dict.update({\"optimizer\": pl_module.optimizers().optimizer.state_dict()})\n",
    "\n",
    "            # save the lr_scheduler\n",
    "            if pl_module.lr_schedulers() is not None:\n",
    "                save_dict.update({\"lr_scheduler\": pl_module.lr_schedulers().state_dict()})\n",
    "\n",
    "            formatted_filename = self.filename.format(epoch=trainer.current_epoch, **trainer.callback_metrics)\n",
    "            torch.save(save_dict, os.path.join(self.dirpath, formatted_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteosh-wp19\u001b[0m (\u001b[33mnlp-assignment\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftPrompt In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\HADOOP\\Desktop\\NLP\\wandb\\run-20221225_144847-2htophzn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-assignment/SoftTune13B/runs/2htophzn\" target=\"_blank\">optimizer_type=Adam-embedding_n_tokens=125</a></strong> to <a href=\"https://wandb.ai/nlp-assignment/SoftTune13B\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "Downloading: 100%|| 2.63G/2.63G [00:32<00:00, 81.7MB/s]\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:345: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | SoftOPTModelWrapper | 1.3 B \n",
      "----------------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,264.056 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:152: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:106: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HADOOP\\Desktop\\NLP\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 500/500 [10:42:07<00:00, 77.06s/it, loss=0.39, v_num=phzn]       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_loss</td><td>0.40722</td></tr><tr><td>trainer/global_step</td><td>7499</td></tr><tr><td>val_loss</td><td>0.38881</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">optimizer_type=Adam-embedding_n_tokens=125</strong>: <a href=\"https://wandb.ai/nlp-assignment/SoftTune13B/runs/2htophzn\" target=\"_blank\">https://wandb.ai/nlp-assignment/SoftTune13B/runs/2htophzn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221225_144847-2htophzn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-26 01:32:57,093]\u001b[0m Trial 0 finished with value: 0.38881388306617737 and parameters: {'embedding_n_tokens': 125}. Best is trial 0 with value: 0.38881388306617737.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\HADOOP\\Desktop\\NLP\\wandb\\run-20221226_013257-2rg4v2ar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-assignment/SoftTune13B/runs/2rg4v2ar\" target=\"_blank\">optimizer_type=Adam-embedding_n_tokens=144</a></strong> to <a href=\"https://wandb.ai/nlp-assignment/SoftTune13B\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a233796b5026b737\n",
      "Using custom data configuration default-12387731b9302b2b\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | SoftOPTModelWrapper | 1.3 B \n",
      "----------------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,264.212 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 500/500 [11:24:48<00:00, 82.18s/it, loss=0.348, v_num=v2ar]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_loss</td><td>0.36803</td></tr><tr><td>trainer/global_step</td><td>7499</td></tr><tr><td>val_loss</td><td>0.34661</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">optimizer_type=Adam-embedding_n_tokens=144</strong>: <a href=\"https://wandb.ai/nlp-assignment/SoftTune13B/runs/2rg4v2ar\" target=\"_blank\">https://wandb.ai/nlp-assignment/SoftTune13B/runs/2rg4v2ar</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221226_013257-2rg4v2ar\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-26 12:59:15,912]\u001b[0m Trial 1 finished with value: 0.34660622477531433 and parameters: {'embedding_n_tokens': 144}. Best is trial 1 with value: 0.34660622477531433.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 2\n",
      "Best trial:\n",
      "  Value: 0.34660622477531433\n",
      "  Params: \n",
      "    embedding_n_tokens: 144\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# initialisation steps\n",
    "torch.cuda.empty_cache()\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "\n",
    "def objective(trial: Trial):\n",
    "    # clear cache so we don't RuntimeError: CUDA out of memory. Tried to allocate 17.00 GB (GPU 0; 39.59 GiB total capacity; 37.53 GiB already allocated; 22.19 MiB free; 37.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # initialise hyperparameter search\n",
    "    trial_config = dict()\n",
    "\n",
    "    # number of embedding tokens\n",
    "    embedding_n_tokens = trial.suggest_int(\"embedding_n_tokens\", 50, 150)\n",
    "    trial_config[\"embedding_n_tokens\"] = embedding_n_tokens\n",
    "    # optimizers\n",
    "    # optimizer_type = trial.suggest_categorical(\"optimizer_type\", [\"Adam\", \"SGD\"])\n",
    "    optimizer_type = \"Adam\"\n",
    "    trial_config[\"optimizer_type\"] = optimizer_type\n",
    "\n",
    "    # override default params with the hyperparamters being searched for\n",
    "    run = wandb.init(project=\"SoftTune13B\",\n",
    "                     name=f\"optimizer_type={optimizer_type}-embedding_n_tokens={embedding_n_tokens}\")\n",
    "    with run:\n",
    "        wandb.config.update(trial_config, allow_val_change=True)\n",
    "\n",
    "        datamodule = DataCombiner(wandb.config[\"model_name\"], batch_size=wandb.config[\"batch_size\"],\n",
    "                                            steps_per_epoch=wandb.config[\"steps_per_epoch\"],\n",
    "                                            datamodules=[PARABANK_Dataset, PARANMT_50M_DATASET],\n",
    "                                            probabilities=[0.5, 0.5])\n",
    "        datamodule.setup()\n",
    "\n",
    "        if (wandb.config[\"load_from_checkpoint\"] is not None) and (os.path.isfile(wandb.config[\"load_from_checkpoint\"])):\n",
    "            model = ParaphraseOPT.load_from_custom_save(wandb.config[\"model_name\"],\n",
    "                                                        wandb.config[\"load_from_checkpoint\"])\n",
    "        else:\n",
    "            model = ParaphraseOPT(wandb.config[\"model_name\"])\n",
    "\n",
    "        checkpoint_callback = SpecificLayersCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=wandb.config[\"checkpoint_save_dir\"],\n",
    "            filename=\"soft-opt-epoch={epoch:03d}-val_loss={val_loss:.3f}\" +\n",
    "                     f\"-optimizer_type={optimizer_type}-embedding_n_tokens={embedding_n_tokens}\" + \".ckpt\",\n",
    "            every_n_epochs=wandb.config[\"checkpoint_every_n_epochs\"],\n",
    "            layers_to_save=wandb.config[\"layers_to_optimize\"]\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "\n",
    "        # create wandb logger (obviously)\n",
    "        wandb_logger = WandbLogger(checkpoint_callback=False)\n",
    "\n",
    "        print(\"TRAINING MODEL\")\n",
    "        trainer = Trainer(max_epochs=wandb.config[\"max_epochs\"], gpus=AVAIL_GPUS,\n",
    "                          check_val_every_n_epoch=wandb.config[\"check_val_every_n_epoch\"],\n",
    "                          callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "                          logger=wandb_logger)\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pruner: optuna.pruners.BasePruner = optuna.pruners.MedianPruner(n_warmup_steps=1000)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=2)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BART METRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2617740631103516, -2.824507474899292]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torchmetrics import Metric\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "\n",
    "class BartScore(Metric):\n",
    "    \"\"\"\n",
    "    Torchmetric version of BartScore as adapted from their github\n",
    "    https://github.com/neulab/BARTScore\n",
    "    With lots of reference to bertscore implementation\n",
    "    https://github.com/PyTorchLightning/metrics/blob/master/torchmetrics/text/bert.py#L40-L235\n",
    "    Compute the score by:\n",
    "    ```\n",
    "    bartscore = BartScore()\n",
    "    score = bartscore(['This is interesting.', 'This is a good idea.'], ['This is fun.', 'Sounds like a good idea.'])\n",
    "    ```\n",
    "    and it should return [-2.152808666229248, -2.948076009750366].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn',\n",
    "                 **kwargs: Dict[str, Any]):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up model\n",
    "        self.device_ = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Set up loss\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Set up metric state variables which keep track of state on each call of update\n",
    "        self.add_state(\"src_input_ids\", [], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"src_attention_mask\", [], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target_input_ids\", [], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target_attention_mask\", [], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: List[str], target: List[str]) -> None:\n",
    "        # dict of 2d list of tensors [batch_size, input_size] although input_size is not fixed\n",
    "        encoded_src = self.tokenizer(preds, padding=True, return_tensors='pt')\n",
    "        encoded_targets = self.tokenizer(target, padding=True, return_tensors='pt')\n",
    "\n",
    "        # 3d list of 2d tensors, since default values of state variables can only be lists or tensors\n",
    "        self.src_input_ids.append(encoded_src['input_ids'])\n",
    "        self.src_attention_mask.append(encoded_src['attention_mask'])\n",
    "        self.target_input_ids.append(encoded_targets['input_ids'])\n",
    "        self.target_attention_mask.append(encoded_targets['attention_mask'])\n",
    "\n",
    "    def compute(self):\n",
    "        score_list = []\n",
    "\n",
    "        src_tokens = self.src_input_ids[0].to(self.device_)\n",
    "        src_mask = self.src_attention_mask[0].to(self.device_)\n",
    "\n",
    "        tgt_tokens = self.target_input_ids[0].to(self.device_)\n",
    "        tgt_mask = self.target_attention_mask[0]\n",
    "        tgt_len = tgt_mask.sum(dim=1).to(self.device_)\n",
    "\n",
    "        # while we do not use the loss computation as a result of labels being provided, the labels also cause\n",
    "        # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bart/modeling_bart.py#L1320\n",
    "        # the decoder input id to be shifted to the right for us, which is needed for this to work\n",
    "        output = self.model(\n",
    "            input_ids=src_tokens,\n",
    "            attention_mask=src_mask,\n",
    "            labels=tgt_tokens\n",
    "        )\n",
    "\n",
    "        # loss calculation based on original bart_score\n",
    "        logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "        lsm_output = self.lsm(logits)\n",
    "        loss = self.loss_fct(lsm_output, tgt_tokens.view(-1))\n",
    "        loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "        loss = loss.sum(dim=1) / tgt_len\n",
    "        curr_score_list = [-x.item() for x in loss]\n",
    "        score_list += curr_score_list\n",
    "\n",
    "        return score_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    bartscore = BartScore()\n",
    "    score = bartscore(['This is interesting.', 'This is interesting.'],\n",
    "                      ['This is very curious.', 'This is incredibly strange.'])\n",
    "    print(score)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate prediction sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df=pd.read_pickle(r\"C:\\Users\\HADOOP\\Desktop\\NLP\\EVALUATION\\Preds\\soft-samples=500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>and we shall call him john .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>i musthave loved you for years , only ... ... i was such a fool , i did n't know it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>allison : terry knows no-one said anything ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>You must be older than I thought.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>these guys manipulate markets , and they create insurgencies to start wars .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td></td>\n",
       "      <td>where a reference is made to this paragraph , article 5 of regulation ( eu ) no 182/2011 shall apply .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td></td>\n",
       "      <td>Athletics at the 2012 Summer Olympics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td></td>\n",
       "      <td>there was n't enough time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td></td>\n",
       "      <td>United States at the 1956 Winter Olympics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td></td>\n",
       "      <td>Your faith will be rewarded.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    preds  \\\n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "..    ...   \n",
       "495         \n",
       "496         \n",
       "497         \n",
       "498         \n",
       "499         \n",
       "\n",
       "                                                                                                        src  \n",
       "0                                                                              and we shall call him john .  \n",
       "1                     i musthave loved you for years , only ... ... i was such a fool , i did n't know it .  \n",
       "2                                                              allison : terry knows no-one said anything ?  \n",
       "3                                                                         You must be older than I thought.  \n",
       "4                              these guys manipulate markets , and they create insurgencies to start wars .  \n",
       "..                                                                                                      ...  \n",
       "495  where a reference is made to this paragraph , article 5 of regulation ( eu ) no 182/2011 shall apply .  \n",
       "496                                                                   Athletics at the 2012 Summer Olympics  \n",
       "497                                                                             there was n't enough time .  \n",
       "498                                                               United States at the 1956 Winter Olympics  \n",
       "499                                                                            Your faith will be rewarded.  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3rlrjez8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">benchmark_run</strong>: <a href=\"https://wandb.ai/nlp-assignment/soft0opt13b/runs/3rlrjez8\" target=\"_blank\">https://wandb.ai/nlp-assignment/soft0opt13b/runs/3rlrjez8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221228_173238-3rlrjez8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3rlrjez8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\HADOOP\\Desktop\\NLP\\wandb\\run-20221228_173908-279mi7bx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-assignment/uncategorized/runs/279mi7bx\" target=\"_blank\">fearless-moon-7</a></strong> to <a href=\"https://wandb.ai/nlp-assignment/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['mur',\n",
       " '',\n",
       " ' Mang',\n",
       " 'sha',\n",
       " ' Silver',\n",
       " 'ze',\n",
       " 'HI',\n",
       " 'aby',\n",
       " ' Nanto',\n",
       " ' forward',\n",
       " '',\n",
       " 'ub',\n",
       " 'g',\n",
       " 'ood',\n",
       " 'ty',\n",
       " 'acht',\n",
       " ';;;;',\n",
       " ' special',\n",
       " 'cool',\n",
       " ' ',\n",
       " ' RE',\n",
       " 'ial',\n",
       " 'town',\n",
       " 'ces',\n",
       " '',\n",
       " 'se',\n",
       " ' board',\n",
       " '////////////////////////////////',\n",
       " ' realistic',\n",
       " ' Licensed',\n",
       " ' prescribing',\n",
       " ' Rome',\n",
       " 'lig',\n",
       " ' proportion',\n",
       " ' open',\n",
       " ' endif',\n",
       " 'burgh',\n",
       " ' awa',\n",
       " 'Supporters',\n",
       " ' fro',\n",
       " ' Today',\n",
       " ' Public',\n",
       " ' Prom',\n",
       " ' Marshall',\n",
       " ' destroyer',\n",
       " ' m',\n",
       " 'eries',\n",
       " '.',\n",
       " ' Sig',\n",
       " ' Par',\n",
       " 'itching',\n",
       " ' need',\n",
       " 'pers',\n",
       " ' U',\n",
       " ' missed',\n",
       " ' gathering',\n",
       " 'roy',\n",
       " 'Reviewer',\n",
       " ' seriously',\n",
       " 'igh',\n",
       " 'age',\n",
       " ' Berg',\n",
       " 'TA',\n",
       " 'enge',\n",
       " ' pick',\n",
       " ' mileage',\n",
       " ' he',\n",
       " ' play',\n",
       " ' sufficient',\n",
       " 'or',\n",
       " ' filmed',\n",
       " 'ne',\n",
       " ' key',\n",
       " 'stone',\n",
       " 'ova',\n",
       " 'elt',\n",
       " ' 1000',\n",
       " ' sol',\n",
       " ' Que',\n",
       " ' Blues',\n",
       " 'everal',\n",
       " ' fer',\n",
       " 'hi',\n",
       " 'iru',\n",
       " 'Grand',\n",
       " ' vide',\n",
       " 'ysc',\n",
       " ' Constitutional',\n",
       " 'handed',\n",
       " ' Talk',\n",
       " ' Wave',\n",
       " 'cia',\n",
       " ' Prin',\n",
       " ' Brom',\n",
       " 'land',\n",
       " ' swing',\n",
       " 'BS',\n",
       " ' kind',\n",
       " 'ter',\n",
       " 'ee',\n",
       " 'lesh',\n",
       " ' distinction',\n",
       " ' fig',\n",
       " ' environmentalists',\n",
       " 'usalem',\n",
       " 'alg',\n",
       " 're',\n",
       " ' bars',\n",
       " 'imir',\n",
       " 'uce',\n",
       " 'gha',\n",
       " 'ute',\n",
       " ' bank',\n",
       " 'des',\n",
       " ' train',\n",
       " ' Opening',\n",
       " ' working',\n",
       " ' interactive',\n",
       " ' no',\n",
       " ' managing',\n",
       " 'Bul',\n",
       " 'ists',\n",
       " '\\n',\n",
       " 'bo',\n",
       " ' Bows',\n",
       " ' monthly',\n",
       " ' infl',\n",
       " 'aper',\n",
       " 'om',\n",
       " 'Force',\n",
       " ' prod',\n",
       " ' All',\n",
       " ' foot',\n",
       " ' clients',\n",
       " 'otor',\n",
       " 'eless',\n",
       " ' Kad',\n",
       " 'obs',\n",
       " ' north',\n",
       " ' preferred',\n",
       " ' domestic',\n",
       " ' code',\n",
       " ' doping',\n",
       " ' side']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\"See embedding\")\n",
    "def reconstruct_tokens(model):\n",
    "    \"\"\"\n",
    "    Find the nearest tokens in the embedding space.\n",
    "    https://stackoverflow.com/questions/64523788/how-to-invert-a-pytorch-embedding\n",
    "    \"\"\"\n",
    "    embeddings = model.model.soft_embedding.wte\n",
    "    learned_embedding = model.model.soft_embedding.learned_embedding\n",
    "\n",
    "    reconstructed = list()\n",
    "    for i in learned_embedding:\n",
    "        distance = torch.norm(embeddings.weight.detach() - i, dim=1)\n",
    "        nearest = torch.argmin(distance)\n",
    "        reconstructed.append(nearest.item())\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "model = ParaphraseOPT.load_from_custom_save(\"facebook/opt-1.3b\",\n",
    "r\"C:\\Users\\HADOOP\\Desktop\\NLP\\training_checkpoints\\07-06-2022-optimize\\soft-opt-epoch=029-val_loss=0.347-optimizer_type=Adam-embedding_n_tokens=144.ckpt\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "tokens = reconstruct_tokens(model)\n",
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "model_prompt_key = {'OPT1.3B Prompt Fine Tuned': lambda x: x + \"</s>\",\n",
    "                    'OPT1.3B Base Model': lambda x: x}\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(model_type: str, prompt: str):\n",
    "    soft_prompt = model_prompt_key[model_type](prompt)\n",
    "    encoded_inputs = tokenizer(soft_prompt, return_tensors=\"pt\")\n",
    "    return encoded_inputs\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "\n",
    "def predict(model_type: str, prompt: str, max_len: int):\n",
    "    inputs = tokenize(model_type, prompt).to(\"cuda\")\n",
    "    if model_type == \"OPT1.3B Base Model\":\n",
    "        outputs = model.generate(inputs.input_ids, max_length=max_len, use_cache=False)\n",
    "    else:\n",
    "        outputs = model.model.generate(inputs.input_ids, max_length=max_len, use_cache=False)\n",
    "    outputs = outputs[:, inputs['input_ids'].size(dim=-1):]\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 52, but `max_length` is set to 45. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 81, but `max_length` is set to 45. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tuparr=[]\n",
    "with open(r'C:\\Users\\HADOOP\\Desktop\\NLP\\para-nmt-5m-processed\\para-nmt-5m-processed.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        source = line.split(\"\\t\")[0]\n",
    "        prediction = predict('OPT1.3B Prompt Fine Tuned', source, 45)\n",
    "        no_punct1 = source.translate(str.maketrans('', '', string.punctuation))\n",
    "        no_punct2 = prediction.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        if(source != prediction):\n",
    "            if (len(word_tokenize(no_punct1)) != len(word_tokenize(no_punct2))):\n",
    "                tuparr.append((source, prediction))\n",
    "                print(len(tuparr))\n",
    "        if len(tuparr) == 500:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"so , unless that 's gon na be feasible , then ...\",\n",
       "  \"unless that's gon'be possible, then...\"),\n",
       " (\"by now , singh 's probably been arrested .\",\n",
       "  \"now, singh's probably been arrested.\"),\n",
       " (' why not ?', ' why not? '),\n",
       " (\"an old man 's mistake  '\", \"an old man's mistake.\"),\n",
       " ('he reached for a microphone and i could hear his voice booming faintly throughout the ship .',\n",
       "  'he reached for a microphone and i could hear his voice booming through the ship.'),\n",
       " ('he loved that little man , by the way .', 'he loved that little man.'),\n",
       " (\"i 've been very , very lucky .\", \"i've been very lucky.\"),\n",
       " (\"well , then , i 'm kind of gon na rub this dinner in her face next time i see her .\",\n",
       "  \"well, i'm going to rub this dinner in her face next time i see her.\"),\n",
       " (\"it 's like , gross , yet oddly delicious .\",\n",
       "  \"it's like, gross, yet oddly delicious.\"),\n",
       " (\"not as much as i 'd like\", \"not as much as i 'd like to see.\"),\n",
       " (\"there 's no doubt in my mind that this is the right group of men but we still have to prove it to the ministry .\",\n",
       "  \"there's no doubt in my mind that this is the right group of men\"),\n",
       " (\"no short colin powell haircut , havin'-ass motherfucker . get  im !\",\n",
       "  \"no short colin powell haircut, havin' ass motherfucker. get  im!\"),\n",
       " (\"it 's so hard . mmm . and there 's a baby growing inside .\",\n",
       "  \"it's so hard. mmm. and there's a baby growing inside.\"),\n",
       " ('none of us likes to believe a servant of father church can be in error , and for those of the clergy there is an added dimension .',\n",
       "  'none of us likes to believe a servant of father church can be wrong,'),\n",
       " (\"'up ! ' he commanded .\", \"'up!'he commanded.\"),\n",
       " (\"i dunno what world you live in , what your game is , what you 're into , but ... i think i 'm done here .\",\n",
       "  \"i'm not sure what world you live in, what your game\"),\n",
       " ('goddamn it , 57 !', '57!'),\n",
       " (' now i am mephisto .', ' i am mephisto.'),\n",
       " ('december 12 , 1973 he made out a christmas list the night before ( drunk ) and was now downtown filling an abridged version .',\n",
       "  'december 12, 1973 he made out a christmas list the night'),\n",
       " (\"`` how ? '' asked gimlet , seeing no hiding place on the flat plain .\",\n",
       "  \"`` how? '' he asked gimlet, seeing no hiding place on the flat plain.\"),\n",
       " (\"you should spend some time taking a personal inventory of what you 're doing for the betterment of society .\",\n",
       "  \"you should take a look at your life and see what you're doing for the betterment of society\"),\n",
       " ('oh , vince ... you know , how every cop always leaves his house with a gun ?',\n",
       "  'how every cop always leaves his house with a gun?'),\n",
       " (\"and me , i 've got my old singin ' partner back .\",\n",
       "  \"and i've got my old partner back.\"),\n",
       " (\"i 'm assuming .\", \"i'm assuming.\"),\n",
       " ('but they are often sentimental about churchill .',\n",
       "  \"but they're often sentimental about churchill.\"),\n",
       " (\"i 'll cum on my cat 's face .\", \"i 'll cum on my cat's face.\"),\n",
       " (\"bob and i had a little chat ... and we 're thinking we 're gon na organize a handling run for you .\",\n",
       "  \"bob and i had a little chat, and we're thinking we're\"),\n",
       " (\"we ca n't wear gloves to do it because that would be seen as insulting .\",\n",
       "  \"we can't wear gloves to do it because it would be seen as insulting.\"),\n",
       " ('under the current rules , vat on natural gas and electricity is levied at the place where they are actually consumed by the customer .',\n",
       "  'under the current rules, vat on natural gas and electricity is levied at the place'),\n",
       " (\"that 's not even a lake . that 's luden pond .\",\n",
       "  \"that's not even a lake. that's luden pond.\"),\n",
       " (\"just like that -- as you 're telling him this , you realize that you 're attracted to him also .\",\n",
       "  \"as you're telling him this, you realize that you're attracted to him also.\"),\n",
       " ('i am 40 years old .', \"i'm 40 years old.\"),\n",
       " (\"she screwed up and she ca n't admit the fact .\",\n",
       "  \"she screwed up and she can't admit it.\"),\n",
       " (\"we 're still doing that , right ?\", \"we're still doing that, right?\"),\n",
       " (\"she 's gon na be taking over for me , starting next week .\",\n",
       "  \"she's gonna be taking over for me, starting next week.\"),\n",
       " ('having seen to the needs of my car , i walked into the office to get those glasses and pay for the gas .',\n",
       "  'i went into the office to get the glasses and pay for the gas.'),\n",
       " ('that i say nothing about what i see , what i hear ?',\n",
       "  'i say nothing about what i see, what i hear?'),\n",
       " ('eu/1/00/146/036 100 x 1 tablets',\n",
       "  'eu/1/00/146/036 100 x 1 tablets 100 x 1'),\n",
       " (\"but-but i-i can get him to tell me who 's making this stuff .\",\n",
       "  \"but i can get him to tell me who's making this stuff.\"),\n",
       " (\"i do n't know how many i 've decorated today ...\",\n",
       "  \"i don 't know how many i've decorated today.\"),\n",
       " (\"as soon as i 'm finished , i can try again .\", \"i'm going to try again.\"),\n",
       " (\"i reckon you 've got a right , major .\",\n",
       "  \"i'm sure you've got a right, major.\"),\n",
       " ('`` what we did then instead was this :',\n",
       "  \"`` we did not do anything then, we just did not do anything. ''\"),\n",
       " (\"but i didn'tdo nothin ' .\", \"but i didn 't do anything.\"),\n",
       " (\"but why am i telling you this ? if you missed out , you 're too late .\",\n",
       "  \"but why am i telling you this? if you missed out, you're too late.\"),\n",
       " ('uh , my kitchen sink is clogged and my boyfriend is kind of useless , so ...',\n",
       "  'my kitchen sink is clogged and my boyfriend is useless, so...'),\n",
       " (\"it 's like my stomach ties up in knots . and i ca n't breathe . and sometimes i think i 'm gon na throw up .\",\n",
       "  \"it's like my stomach is tied up in knots. and\"),\n",
       " (\"select sid from shared_sessions where sid = 'bcdd1884d7777f2f888fae206241f2bb ' in /var/www/tutor/includes/database.mysql.inc on line 120\",\n",
       "  'select'),\n",
       " ('best you can , mike . - yes , sir .', 'best you can, mike.'),\n",
       " ('later', 'later, later, later.'),\n",
       " ('just wait till you go get the hamburgers . all right ?',\n",
       "  'wait till you get the hamburgers.'),\n",
       " (\"oh , well . i do n't care about anything , anyway .\",\n",
       "  \"i don 't care about anything, anyway.\"),\n",
       " ('the opening of the spa season is traditionally connected with blessing the springs at the colonnade and speeches of the town and the spa company representatives .',\n",
       "  'the opening of the spa season is traditionally connected with the blessing of the'),\n",
       " (\"it 's like the family pet .\", \"it's like the family pet.\"),\n",
       " (\"`` it should n't be your responsibility .\",\n",
       "  \"`` it's not your responsibility. ''\"),\n",
       " (\"me ? i 'm not talking about me .\",\n",
       "  \"i'm not talking about me. i'm not talking about me.\"),\n",
       " (\"so you 're telling me everything you 've done -- taking the 4400 , sending them back . -- it has n't changed anything ?\",\n",
       "  \"so you're telling me that you've done everything you '\"),\n",
       " ('thank you , master of the bloody obvious .',\n",
       "  'thank you, master of the obvious.'),\n",
       " (\"hey , there we go . hey , all we need now is pizza . we 've get ourselves one hell of the evening\",\n",
       "  \"hey, we've got ourselves a hell of a night. we've got ourselves\"),\n",
       " (\"it 's almost too much juxtaposition for me .\", \"it's too much for me.\"),\n",
       " (\"fm seeing what he 's doing now , she thought .\",\n",
       "  \"fm seeing what he's doing now, she thought.\"),\n",
       " ('fifteen minutes past midnight , evers got out of his car beside his home in a negro residential area .',\n",
       "  'fifteen minutes past midnight, evers got out of his car in a negro neighborhood.'),\n",
       " (\"all right . let 's go .\", \"let's go. let's go.\"),\n",
       " (\"that 's his dead son 's birthday .\", \"that's his dead son's birthday.\"),\n",
       " (\"so why not just pack up all your belongings and take a chance in the badlands , 'cause this town is done .\",\n",
       "  \"so why not just pack up and go to the badlands, 'cause this town is\"),\n",
       " ('then he must leave . well , perhaps by then you will have seen the truth .',\n",
       "  \"then he must leave. maybe by then you 'll have seen the truth.\"),\n",
       " (\"um ... forever . and you , why are you sneaking out here in the middle of the night if you 're not scared ?\",\n",
       "  \"you're not scared? you're not scared?\"),\n",
       " (\"you 're a good man , johnson .\", \"you're a good man, johnson.\"),\n",
       " (\"yeah . we 'd sleep in , then go to church .\",\n",
       "  \"we 'd sleep in, then go to church.\"),\n",
       " ('only in the most northern zone , the pec/pnec ratio ( 10 ) using the mean concentration is below 1 .',\n",
       "  'only in the most northern zone, the pec/pnec ratio ( 10 ) is'),\n",
       " ('oh , my gosh ...',\n",
       "  'oh, my gosh... my god... my god... oh, my god... oh, my god... oh, my god... oh, my god... oh, my god...'),\n",
       " (\"you know , he 'd be pretty damn mad if he knew i was talking to you .\",\n",
       "  \"he 'd be mad if he knew i was talking to you.\"),\n",
       " (\"your boy al , he 's pixilated or something . he wo n't listen to reason .\",\n",
       "  \"your boy al, he's pixilated or something. he didn 't listen to reason.\"),\n",
       " ('i should let you get back to sleep .', 'i should let you go to sleep.'),\n",
       " (\"i 'm sorry , i do n't mean to tell you what you should ask anyone , i would never dare , but ...\",\n",
       "  \"i'm sorry, i don 't mean to tell you what you should ask\"),\n",
       " ('the commission would though point out that this preliminary conclusion is based on a certain number of assumptions .',\n",
       "  'the commission would point out that the preliminary conclusion is based on certain assumptions.'),\n",
       " (\"all i 'm saying is do n't go trying to be a hockey player .\",\n",
       "  \"i'm just saying, don 't try to be a hockey player.\"),\n",
       " (\"nah , i do n't go out with anyone from the firm .\",\n",
       "  \"i don 't go out with anyone from the firm.\"),\n",
       " (\"she tried , with all her telepathic skill , to reach her cousin 's mind , but all she could feel was horror and long dread .\",\n",
       "  \"she tried to reach her cousin's mind, but all she could\"),\n",
       " (\"i 'll reach you later .\", \"i 'll get in touch with you later.\"),\n",
       " (\"i 'm iust not sure i 'm ready yet , dad , afer all that 's happened this year and-\",\n",
       "  \"i'm not sure i'm ready yet, dad, after all that '\"),\n",
       " ('you must have done one hell of a tony robbins on those people to get them to hand over their kids .',\n",
       "  'you must have done a tony robbins on those people to get them to hand'),\n",
       " (\"we ca n't stay on this ship . you know that .\",\n",
       "  \"we can't stay on this ship. you know that.\"),\n",
       " ('i propelled the corpse toward my attacker with all my strength and did not wait to observe the result of my action .',\n",
       "  'i pushed the corpse toward my attacker with all my strength and did not wait to observe the result of'),\n",
       " (\"i should 've brought her in sooner ... the first time she threw up .\",\n",
       "  \"i should've brought her in sooner.\"),\n",
       " (\"let 's go , people .\", \"let's go, people.\"),\n",
       " (\"but i know why i 'm here . to be a pilot .\",\n",
       "  \"but i know why i'm here. to be a pilot.\"),\n",
       " (\"if they do n't hear his voice in the next 30 minutes ,\",\n",
       "  \"if they don 't hear his voice in the next 30 minutes, they 'll be dead.\"),\n",
       " ('we do have things to do .', 'we have things to do.'),\n",
       " ('speaking of great honeymooning couples , charlie and harriet mackenzie .',\n",
       "  'speaking of honeymooning couples, charlie and harriet mackenzie.'),\n",
       " ('and , um , we can do your knuckles and your back , your legs ... ahh , sugar , sugar ... your arse .',\n",
       "  'and, uh, we can do your knuckles and your back, your'),\n",
       " (\"`` you wo n't have a street full of shoppers , `` or sidewalk cafes that cars that can go through . ''\",\n",
       "  \"`` you 'd have a street full of shoppers, `` or a sidewalk cafe that cars can\"),\n",
       " ('look , i want you to have this football .',\n",
       "  'i want you to have this football.'),\n",
       " ('please , this is what best friends do ... right before their friendship ends . why did you agree to let her stay ?',\n",
       "  'please, this is what friends do before their friendship ends. why did you agree to let her'),\n",
       " ('thanks for crossing me . no problem .',\n",
       "  'thanks for crossing me. no problem. no problem. no problem. no problem. no problem. no problem. no problem. no problem. no problem. no problem.'),\n",
       " ('a little old lady in a nissan micra . gone into the back of a land rover .',\n",
       "  'an old lady in a nissan micra. gone into the back of a land rover.'),\n",
       " (\"emolia , i 'm the one who said those things about sebastio . it was n't vic .\",\n",
       "  \"emilia, i'm the one who said those things about sebastio. it\"),\n",
       " ('they are simply not interested in fighting for a group of prospective clients , which university students undoubtedly are long-term .',\n",
       "  'they are not interested in fighting for a group of prospective clients, which university students are long-term'),\n",
       " ('we have a room full of computers dedicated to finding workable patterns .',\n",
       "  'we have a room full of computers that are dedicated to finding patterns.'),\n",
       " (\"all right , i 'm coming out !\", \"i'm coming out!\"),\n",
       " ('why are they carrying dead cat pictures , huh ?',\n",
       "  'why are they carrying dead cats?'),\n",
       " (\"'born bergen , norway ... '\", \"born in Bergen, Norway... '\"),\n",
       " (\"it 's a tearful good-bye to her true love , norman blurder , the rural juror .\",\n",
       "  \"it's a tearful good-bye to norman blurder, the rural juror\"),\n",
       " (\"you know , i honestly do n't have the experience .\",\n",
       "  \"i'm not sure i have the experience.\"),\n",
       " ('underwater caving is notoriously dangerous .',\n",
       "  'underwater caving is dangerous.'),\n",
       " ('get away from me ! fucking legs !', 'get away from me!'),\n",
       " (\"where 're they !\", \"where're they?\"),\n",
       " ('of a dedicated service within the agency',\n",
       "  'a dedicated service within the agency.'),\n",
       " (\"lila , i 've got to go .\", \"lila, i've got to go.\"),\n",
       " ('uh , this isn  t my shirt .', 'this is not my shirt.'),\n",
       " ('thomas could only hope they  d make some sense of everything .',\n",
       "  \"thomas could only hope they 'd understand.\"),\n",
       " ('however , it is likely to be well above the eu average .',\n",
       "  'however, it is likely to be above the average.'),\n",
       " (\"`` listen , dr. van allen , we 've got no business talking about this on the radio .\",\n",
       "  \"`` listen, dr. van allen, we've got no business talking about this on the radio\"),\n",
       " (\"the queen 's reign ends today .\", \"the queen's reign ends today.\"),\n",
       " (\"that 's cute . a zombie with a gas attack .\",\n",
       "  \"that's cute. a zombie with a gas attack.\"),\n",
       " (\"my mom 's staying with family upstate , but i 've got to find someplace around here to crash .\",\n",
       "  \"my mom's staying with family upstate, but i've got to find someplace\"),\n",
       " ('they waited ... she gives birth in prison then they let her go .',\n",
       "  'they waited for her to give birth in prison, then they let her go.'),\n",
       " ('the relative orientation of each methyl group ( ch',\n",
       "  'the relative orientation of each methyl group ( methyl group ) ( ) ( ) ( )'),\n",
       " ('he caught himself , and cleared his throat self-consciously .',\n",
       "  'he caught himself, and cleared his throat.'),\n",
       " (\"`` it is n't 'for ' anything , '' he said thoughtfully .\",\n",
       "  \"`` it's not for anything, '' he said thoughtfully.\"),\n",
       " (\"`` bevey , '' she said , `` i want you to find an orthopedic pediatric surgeon who will operate on szuzannah . ''\",\n",
       "  \"`` bevey, '' she said, `` i want you to\"),\n",
       " ('and lucky for him and lucky for us , before that earthquake there was actually a bunch of seismographic stations already in the area .',\n",
       "  'and lucky for him and lucky for us, before the earthquake there was already a bunch'),\n",
       " ('i have to keep going on till the end in order to grab hold of the future i want .',\n",
       "  'i have to keep going until the end in order to grab hold of the future i want.'),\n",
       " ('look , when you sell a hammer , someone can be killed with it .',\n",
       "  'when you sell a hammer, someone can be killed with it.'),\n",
       " (\"i 'm a little surprised .\", \"i'm a little surprised.\"),\n",
       " ('yeah , i want to check my phone , see if it still rings .',\n",
       "  'i want to check my phone, see if it still rings.'),\n",
       " (\"all right , look , if it 's true . turn it around .\",\n",
       "  \"if it's true, turn it around. turn it around. turn it around. turn it around. turn it around. turn it\"),\n",
       " (\"looks like you 're doing all right for yourself , are n't you ?\",\n",
       "  \"you're doing fine for yourself, are you?\"),\n",
       " ('failed to start x server several times in a short time period ; disabling display % s',\n",
       "  'failed to start x server several times in a short time period ; disabling % s.'),\n",
       " (\"holly told me where you 're coming from , and i 'm thinking now is not the best time .\",\n",
       "  \"holly told me where you're coming from, and i'm thinking now is not the\"),\n",
       " (\"okay , so if i find someone with better credentials , who does n't look like `` emmie , '' you 'll be just as excited ?\",\n",
       "  \"if i find someone who's better than emmie,\"),\n",
       " ('i/o address conflict detected .', 'i/o conflict detected.'),\n",
       " (\"there 's got i 'm overlooking .\",\n",
       "  \"there's a lot of things i'm overlooking.\"),\n",
       " ('fortunately for ukraine , its business is private , and a common view of the whole business community is that the electoral rerun must be conclusive .',\n",
       "  \"fortunately, the ukraine's business is private, and\"),\n",
       " ('i guess i just always had a crush on an older man .',\n",
       "  'i guess i always had a crush on an older man.'),\n",
       " ('barbara : you can not expect me to sit down with a complete stranger , a , a man .',\n",
       "  \"barbara : i'm not going to sit down with a stranger, a man.\"),\n",
       " (\"it sounds like no one 's ever found the fifth cache .\",\n",
       "  \"it's like no one has ever found the fifth cache.\"),\n",
       " ('says one of the men originally sent to find it ... on a submarine called the zeus faber .',\n",
       "  'says one of the men who was sent to find it... on a submarine called the zeus fab'),\n",
       " (\"now you 're getting warmer . - clues are all there .\",\n",
       "  \"now you're getting warmer. - clues are all there.\"),\n",
       " ('dd moved about the camp , starting fires , getting out pans , straightening tents , preparing everything for their return .',\n",
       "  'dd moved about the camp, starting fires, getting out pans, straightening tents, preparing everything for'),\n",
       " ('i had to oversee gift bags .', 'i had to oversee the gift bags.'),\n",
       " (\"`` i 'm scared of boredom , still i 'm bored ''\",\n",
       "  \"`` i'm scared of boredom, still i'm bored ''\"),\n",
       " ('that poor girl is carrying around ... a sadistic little mustachioed shit inside her .',\n",
       "  'that poor girl is carrying around a little mustache-faced shit in her.'),\n",
       " (\"we 're expanding into the north east , and i 'm here to set it up .\",\n",
       "  \"we're expanding into the north east, and i'm here to set it up.\"),\n",
       " ('it was the face of a young magic-user , weary from long nights of study at his books , but now relaxed , finding welcome rest .',\n",
       "  'it was the face of a young magic-user, weary from long'),\n",
       " (\"i 'm sure you 're gon na figure it out .\",\n",
       "  \"i'm sure you 'll figure it out.\"),\n",
       " (\"the secret 's safe .\", \"the secret's safe.\"),\n",
       " ('2a . for substances referred to in paragraph 2 that are engineered nanomaterials , the following additional conditions shall apply :',\n",
       "  '2a. for substances referred to in paragraph 2 that are engineered nanomaterials,'),\n",
       " ('um , there is a peewee-league soccer playoff game tomorrow on the alpha field .',\n",
       "  'there is a peewee-league soccer playoff game tomorrow on the alpha field.'),\n",
       " ('i really have to get home .', \"i've got to get home.\"),\n",
       " ('forn said ,  in most languages it  s vampires , not vampires .',\n",
       "  \"forn said,  in most languages it's vampires, not vampires.\"),\n",
       " ('this part of it was for such as karl .', 'this part of it was for karl.'),\n",
       " ('well , well . please sit down . - barny .',\n",
       "  'please sit down. barny. please sit down.'),\n",
       " ('the women have been charged on 17 counts of aggravated indecent assault , as zimbabwean law does not recognize the act of a woman raping .',\n",
       "  'the women have been charged on 17 counts of aggravated indecent assault, as z'),\n",
       " (\"i 've got 30 , what have you got ?\", \"i've got 30, what've you got?\"),\n",
       " ('when i entered the premises , i saw the two suspects , the guns the money , the drugs , the dead body .',\n",
       "  'when i entered the premises, i saw the two suspects, the guns, the money, the'),\n",
       " (\"ido n't understand a word of it .\", \"i don 't understand a word of it.\"),\n",
       " (\"call me a doormat -- get it all out of your system -- but i 'm doing it .\",\n",
       "  \"call me a doormat -- get it out of your system -- but i'm doing it.\"),\n",
       " (\"i 'm ok .\", \"i'm fine.\"),\n",
       " (\"well , i do n't hit women ,\", \"i don 't hit women, i don 't hit women.\"),\n",
       " (\"i ca n't believe they 're just letting that woman free .\",\n",
       "  \"i can 't believe they're letting her go.\"),\n",
       " ('[ 9 ] http : //ec.europa.eu/justice/citizen/voting-rights/index_en.htm . [ 10 ] see annex .',\n",
       "  '[ 9 ] http : //ec.'),\n",
       " (\"do you think , like , i 'm too nice to make it as a designer ?\",\n",
       "  \"do you think i'm too nice to be a designer?\"),\n",
       " ('ok , aarons . hit the road .', 'aaron, hit the road.'),\n",
       " (\"she 's playing with fire - he 's not ready for nibbly pig .\",\n",
       "  \"she's playing with fire - he's not ready for nibbly pig.\"),\n",
       " (\"that 's you , right , miss mcnamara ?\", \"that's you, miss mcnamara?\"),\n",
       " ('and ... i think i feel the same way , too .',\n",
       "  \"and i'm sure i'm the same way.\"),\n",
       " ('yeah , it is an appropriate gift for the baby .',\n",
       "  \"yeah, it's a good gift for the baby.\"),\n",
       " ('according to hungary , the commission assessed the legislation at issue in isolation only , without taking the general context of that legislation into account .',\n",
       "  'according to hungary, the commission assessed the legislation at issue in isolation, without'),\n",
       " ('the lab found trace amounts of titanium alloy in the threads .',\n",
       "  'the lab found traces of titanium alloy in the threads.'),\n",
       " (\"it 's a sick feeling .\", \"it's a sick feeling.\"),\n",
       " (\"you came early , but you 're gon na make it .\",\n",
       "  \"you're going to make it, but you're going to make it.\"),\n",
       " ('he bought it when he was quite a young man on his grand tour .',\n",
       "  'he bought it when he was a young man on his grand tour.'),\n",
       " ('oh , man . are you okay ?', 'are you all right?'),\n",
       " ('every time i think about the eulogy , i just ... all i picture is frost laughing at me .',\n",
       "  'every time i think about the eulogy, i just... i just... i just... i just'),\n",
       " (\". were cured today , i would be eighty-two standard years old before she again reached the age she was when we first met . ''\",\n",
       "  \". i 'd be eighty-two years old before she would be eighty\"),\n",
       " ('he doth it as like one of these harlotry players as ever i see .',\n",
       "  'he does it as if he were a harlot.'),\n",
       " ('just do it . if quinn asks , tell him it failed .',\n",
       "  'tell him it failed. if quinn asks, tell him it failed.'),\n",
       " (\"i 'm on a lasagna diet .\", \"i'm on a lasagna diet.\"),\n",
       " (\"now , i 'd be willing to part with just a taste of it , but ...\",\n",
       "  \"i 'd be willing to taste it, but...\"),\n",
       " (\"mick , you 're going to say it was self-defense , right ?\",\n",
       "  \"mick, you're going to say it was self-defense, right?\"),\n",
       " (\"if i 'm right about the other party , you wo n't just be making an obscene amount of money .\",\n",
       "  \"if i'm right, you 'll be making a lot of money.\"),\n",
       " ('so are we going home together or what ?', 'so are we going home or what?'),\n",
       " (\"it 's going awfully slow . - that 's all .\",\n",
       "  \"it's going slow. - that's all.\"),\n",
       " (\"'were you surprised i found the sword ? ' agwaine asked him .\",\n",
       "  \"'were you surprised i found the sword?'agwaine asked him.\"),\n",
       " (\"fuck me , that ai n't bad .\", \"that's not bad.\"),\n",
       " ('yes indeed , all sorts of friends are available for our inspection , but at least one seems to have dropped out of sight .',\n",
       "  'yes, all sorts of friends are available for our inspection, but one seems to have disappeared'),\n",
       " (\"ai n't got no toes ... ... to worry about stomping them .\",\n",
       "  \"i'm not going to stomp my toes.\"),\n",
       " ('brother arshad has a beautiful singing voice .',\n",
       "  'brother arshad has a beautiful voice.'),\n",
       " ('one interested party claimed that the commission did not investigate whether the union industry failed to anticipate that government support schemes would be abruptly withdrawn or decreased .',\n",
       "  'one interested party claimed that the commission did not investigate whether the industry failed to'),\n",
       " ('it feels like yesterday .', \"it's like yesterday.\"),\n",
       " (\"do n't forget , if your tips are magic , she 'll be back for another number with our magician , jim 's etoile .\",\n",
       "  \"don 't forget, if your tips are magic, she\"),\n",
       " (\"no . no , you do n't . you do n't .\",\n",
       "  \"no, you don 't. you don 't. you don 't. you don 't.\"),\n",
       " ('do international tribunals of the sort milosevic faced before his death promote or postpone serious self-reflection and reconciliation in damaged societies ?',\n",
       "  'do tribunals of the kind milosevic faced before his death'),\n",
       " ('finally , the villain is mortally wounded . he dies ... ... in character .',\n",
       "  'the villain is mortally wounded. he dies...... in character.'),\n",
       " (\"i deprive them for only two days , then give them back a pittance and now they 'll eat dung , they really will .\",\n",
       "  'i give them two days, then give them back a pittance, and'),\n",
       " ('subat gaped at him , his expression incredulous .',\n",
       "  'he was incredulous, his expression incredulous.'),\n",
       " (\"no , it 's just me .\", \"no, it's just me.\"),\n",
       " ('i mean , even at half off , the shit is still expensive .',\n",
       "  'even at half off, the shit is still expensive.'),\n",
       " ('the spanish are barely a day away , majesty .',\n",
       "  'the spanish are just a day away.'),\n",
       " (\"what 's she gon na do ?\", \"what's she gonna do?\"),\n",
       " ('did i happen to say thank you ?', 'did i say thank you?'),\n",
       " (\"back to your lesson . - you 're taking it to your room .\",\n",
       "  \"back to your room. - you're going to your room. - you're going to your room. - you're going\"),\n",
       " ('there was the quick thudding of running feet as a number of deserters got there first .',\n",
       "  'there was the thudding of running feet as a number of deserters got there first.'),\n",
       " (\"`` then i suggest you remove yourself from school property at once , young man , before i call the police . ''\",\n",
       "  \"`` i suggest you leave school property immediately, young man, before i call the police. ''\"),\n",
       " (\"he 's dead . who the hell is he ?\", \"he's dead. who the hell is he?\"),\n",
       " ('sure , if you call three weeks ages ago .',\n",
       "  'sure, if you call three weeks ago.'),\n",
       " (\"no , they 're gon na find him , hon .\", \"no, they're gon'find him.\"),\n",
       " (\"i figured this brunch would be so lame , i needed something that would n't upstage them .\",\n",
       "  'i figured this brunch would be lame, i needed something that would not upstage them.'),\n",
       " (\"i 'm going to do what the fuck i want .\", \"i'm going to do what i want.\"),\n",
       " (\"' no , i never heard of any place like that , ' he told me .\",\n",
       "  \"' i've never heard of a place like that.\"),\n",
       " ('now you need to save his .', 'now you need to save his life.'),\n",
       " ('your child comes to this school , we will guarantee that we will get your child into college .',\n",
       "  'your child comes to this school, we will guarantee that he will get into college.'),\n",
       " (\"let them go and dig their own wells . ''\",\n",
       "  \"let them dig their own wells. ''\"),\n",
       " ('it was a great relief to have the car standing on the street again ; the way that rotted bumper bent up under the jack had scared me .',\n",
       "  'it was a relief to have the car back on the street again'),\n",
       " (\"that 's not only childish , it 's irrational , and i do n't have the time to try to reason with somebody whose wits have deserted him .\",\n",
       "  \"that's not only childish, it's\"),\n",
       " (\"i do n't know if i 'd go disturbing him , if i was you .\",\n",
       "  \"i 'dn't want to disturb him, if i were you.\"),\n",
       " ('the professor carefully tried the lock , lest we might not be able to open it from within should we be in a hurry making our exit .',\n",
       "  'the professor carefully tried the lock, lest we might not be able to open'),\n",
       " ('well , uh -- - i can take it . everyone knows how hard it must be for daniel ,',\n",
       "  \"well, i'm sure it's hard for daniel.\"),\n",
       " (\"'trouble is , we do n't have any idea where the float may be . '\",\n",
       "  \"'we don 't know where the float is. '\"),\n",
       " ('john olafsen had made a good living back when life had been normal .',\n",
       "  'john olafsen had made a good living before the war.'),\n",
       " ('they had some on samples for last week .',\n",
       "  'they had some samples for last week.'),\n",
       " (\"'where 's my book ? '\", \"'where's my book? '\"),\n",
       " (\"and they 've awarded him $ 500,000 .\",\n",
       "  \"and they've awarded him $ 500,000.\"),\n",
       " (\"lights ... media ... fireplace ... even turn on the lawn sprinklers if someone is n't supposed to be here .\",\n",
       "  'lights, media, fireplace, even turn on the lawn sprinklers if someone is not supposed to be'),\n",
       " ('let me guess . sumei ?', \"i'm guessing sumei.\"),\n",
       " ('what does merlyn ?', 'what does merlyn do?'),\n",
       " (\"i ca n't think straight if i do n't relax .\",\n",
       "  \"i can't think straight if i don 't relax.\"),\n",
       " ('according to my clinical research on drug induced amnesia ...',\n",
       "  \"according to my clinical research on drug induced amnesia... i've found that the drug is not responsible for the amnesia.\"),\n",
       " ('interviewer : you think if he goes to sweden , he may be sent to the states ?',\n",
       "  'interviewer : if he goes to sweden, he may be sent to the states?'),\n",
       " ('when the cab was arriving , i heard the scream .',\n",
       "  'when the cab arrived, i heard the scream.'),\n",
       " (\"but what 's going on up there ?\", \"but what's going on up there?\"),\n",
       " ('ah , ah , are you a doctor ?', 'are you a doctor?'),\n",
       " (\"i 'm seeing a real nice woman now .\", \"i'm seeing a nice woman now.\"),\n",
       " ('it was bremen , changing his appearance , his form , his very thinking , so that he could penetrate the stronghold of the warlock lord .',\n",
       "  'bremen changed his appearance, his form, his thinking, so that'),\n",
       " ('organised civil society should play a key role but seems to be reluctant to assume this .',\n",
       "  'civil society should play a key role but seems to be reluctant to assume this.'),\n",
       " (\"and i think you know who i 'm talking about . - jack-off ! jack-off !\",\n",
       "  \"and i'm talking about jack-off! jack-off! jack-off! jack-off!\"),\n",
       " (\"uh , pardon me for asking , but , uh , are n't you the sheriff of absaroka county ?\",\n",
       "  'are you the sheriff of absaroka county?'),\n",
       " ('i just think i need ... a couple of months , you know , to finish my exams .',\n",
       "  \"i'm just going to take a little break, you know, to finish my exams.\"),\n",
       " (\"do you think he 'll welcome you , into his arms , like the so very many you 've sent his way ?\",\n",
       "  \"do you think he 'll welcome you, into his arms, like the many you '\"),\n",
       " ('he wished only to be let alone , and perhaps to sleep the night out , and not to be questioned after .',\n",
       "  'he wished only to be left alone, and perhaps to sleep the night out, and not to be'),\n",
       " ('web link to the full text of the aid measure : http : //www.mzcr.cz/odbornik/pages/1163-resortni-program-vyzkumu-a-vyvoje-ministerstva-zdravotnictvi-iii-na-leta-2010-2015-kod-nt.html',\n",
       "  'web'),\n",
       " (\"that 's what makes him one of the most relevant artists of our time .\",\n",
       "  \"that's what makes him one of the most relevant artists of our time.\"),\n",
       " ('article 4 in evaluating the identity of , or confusion with , a variety denomination of another variety , the following shall apply :',\n",
       "  'article 4 in evaluating the identity of, or confusion with, a variety denomination of another variety,'),\n",
       " (\"you 're makin ' me nervous .\", \"you're making me nervous.\"),\n",
       " ('i never had that thought about you , mr ritter .',\n",
       "  'i never thought about you, mr ritter.'),\n",
       " ('gerasimov was thinking , too .', 'gerasimov was thinking.'),\n",
       " ('just resting , sir . just resting .',\n",
       "  'just resting. just resting. just resting.'),\n",
       " (\"i have to open the station early . that 's sad .\",\n",
       "  \"i have to open the station early. that's sad.\"),\n",
       " ('i understand you .', \"i'm sorry.\"),\n",
       " (\"he 's awfully short . and i think he 's talking to himself .\",\n",
       "  \"he's short. and he's talking to himself.\"),\n",
       " (\"we do n't need to play any games with it tonight .\",\n",
       "  \"we don 't need to play any games with it.\"),\n",
       " ('firing off memos in the middle of the night directly to the joint chiefs of staff . which pissed off his immediate superiors to no end .',\n",
       "  'firing off memos in the middle of the night to the joint chiefs of'),\n",
       " ('all of a sudden , i felt this sensation on my neck .',\n",
       "  'i felt a sensation on my neck.'),\n",
       " ('so , applejack .', 'applejack.'),\n",
       " (\"and what 's the difference ?\",\n",
       "  \"and what's the difference between you and me?\"),\n",
       " (\"she did n't show you no papers , no deed nor nothing ?\",\n",
       "  \"she didn 't show you any papers, no deed, nothing?\"),\n",
       " (\"let 's run away .\", \"let's run away.\"),\n",
       " (\"we 'll all go to the chair . - maybe it was n't one of us !\",\n",
       "  \"we 'll all go to the chair. maybe it wasn't one of us!\"),\n",
       " (\"jacob ... i 'm engaged to you .\", \"jacob, i'm engaged to you.\"),\n",
       " (\"they 're savages -- reckless and single-minded .\",\n",
       "  \"they're savages -- reckless and single-minded.\"),\n",
       " ('o , this is hire and salary , not revenge .',\n",
       "  'this is hire and salary, not revenge.'),\n",
       " ('five years ago , patricia bradley was granted ... a conditional release by the governor .',\n",
       "  'five years ago, patricia bradley was granted conditional release by the governor.'),\n",
       " ('relax uncle , sanju just talks rubbish .',\n",
       "  'uncle, sanju just talks rubbish.'),\n",
       " (\"i 'm on the way down .\", \"i'm on my way down.\"),\n",
       " (\"go tell the surgeon to cut out the freak 's horns .\",\n",
       "  \"tell the surgeon to cut out the freak's horns.\"),\n",
       " (\"`` it 's not a secret , '' she cried .\",\n",
       "  \"`` it's not a secret, '' she cried.\"),\n",
       " ('the ceiling collapsed , and aquim threw himself forward , trying to shield the preservation canister and the magnificent brain of the ancient cogitor .',\n",
       "  'the ceiling collapsed, and aquim threw himself forward, trying to shield the'),\n",
       " ('i can be there in about four hours . do you want me to come over ?',\n",
       "  \"i 'll be there in four hours. do you want me to come over?\"),\n",
       " ('in other words , its application requires the taking of positive action , failing which rules conflicting with community law are applied .',\n",
       "  'in other words, the application of the law requires the taking of positive action, failing which rules conflicting'),\n",
       " (\"the telephone was n't working , and anna had n't come down - she 's the housekeeper .\",\n",
       "  'the telephone was not working, and anna had not come down.'),\n",
       " (\"lying atop sayre 's desk were amazingly complete files on all four of them .\",\n",
       "  'on the desk were all the files on the four of them.'),\n",
       " (\"i 'll see you tonight , 9:00 sharp .\", \"i 'll see you at 9:00 p.m. sharp.\"),\n",
       " ('to thank you for helping me clean up the mess that i made in your shop of bugs .',\n",
       "  'thank you for helping me clean up the mess that i made in your shop of bugs.'),\n",
       " (' all right , you incompetent bungling blind microcephalic dingdongs !',\n",
       "  ' you incompetent, blind, dumb, dung-dong-dong-dong-dong-dong'),\n",
       " (\"keepsakes , you might ... excuse me . could you tell me where i 'd go to collect a trunk ?\",\n",
       "  \"keepsakes, you might... excuse me. could you tell me where i 'd go\"),\n",
       " (\"if that 's the case , it must be two seconds .\",\n",
       "  \"if it's true, it must be two seconds.\"),\n",
       " (\"who 's she ?\", \"who's she?\"),\n",
       " (\"no , that 's not what he means .\", \"no, that's not what he means.\"),\n",
       " ('now she would like to meet father moinighan .',\n",
       "  'now she wants to meet her father.'),\n",
       " ('it will be a tracking shot through the wood with plants growing , flowers blooming and the woodland coming to life over the course of a year .',\n",
       "  \"it's a tracking shot through the wood with plants growing, flowers\"),\n",
       " ('time is running out if you decide you want to ...',\n",
       "  'time is running out if you decide to...'),\n",
       " ('for a short while at least , this small patch of scotland could honestly call itself the oil capital of the world .',\n",
       "  'for a short while, this small patch of scotland could call itself the oil capital'),\n",
       " ('`` several times in the last couple of days you yourself were almost killed .',\n",
       "  \"`` you were almost killed several times in the last couple of days. ''\"),\n",
       " (\"it 's our job to try to protect you from making the dangerous ones , if we can .\",\n",
       "  \"it's our job to try to protect you from making the dangerous ones.\"),\n",
       " (\"mom 's been gone for ten years .\", \"mom's been gone for ten years.\"),\n",
       " (\"we do n't even know what you have yet .\", \"we don 't know what you have.\"),\n",
       " ('i never knew that you liked kids so much .',\n",
       "  'i never knew you liked kids so much.'),\n",
       " ('you know what the best thing is about a pimp ?',\n",
       "  \"the best thing about a pimp is that he's got a lot of money.\"),\n",
       " (\"say , these kids , they ca n't all be bad , can they ?\",\n",
       "  \"say, these kids, they can't all be bad, can they?\"),\n",
       " (\"i 'll make sure they 're taken care of .\",\n",
       "  \"i 'll make sure they're taken care of.\"),\n",
       " ('it costs a lot to look that cheap .', \"it's expensive to look like that.\"),\n",
       " (\"uh , aside from a course of antibiotics a year ago , we have n't filled anything for her .\",\n",
       "  \"we've not filled anything for her in a year.\"),\n",
       " (\"my goodness , i would have been disappointed ... if you had n't crushed my hand .\",\n",
       "  'i would have been disappointed if you had not crushed my hand.'),\n",
       " (\"it 's a dessert place right around the corner .\",\n",
       "  \"it's a dessert place right around the corner.\"),\n",
       " (\"it 's just like riding a bike .\", \"it's just like riding a bike.\"),\n",
       " ('in july the eu delegation co-organised a workshop on roma issues which drew up several',\n",
       "  'in july the eu delegation organised a workshop on roma issues.'),\n",
       " (\"it 's been real lonely without you , rita .\",\n",
       "  \"it's been very lonely without you, rita.\"),\n",
       " ('reputation of the  melon de guadeloupe ',\n",
       "  'reputation of the melon de guadeloupe.'),\n",
       " ('perhaps you misunderstoodthe meaning of the term .',\n",
       "  'perhaps you misunderstood the meaning of the term.'),\n",
       " (\"really , he waved at stevie wonder . '' what the fuck ?\",\n",
       "  \"he waved at stevie wonder. '' what the fuck? ''\"),\n",
       " ('by the time it was over , cashed out over 25 grand .',\n",
       "  'it was over 25 grand.'),\n",
       " ('where did this woman run off to now ?', 'where did she go?'),\n",
       " ('you understand i used to be in the nba , yes ?',\n",
       "  'i used to be in the nba, yes?'),\n",
       " (\"indigenous west indies , southern u.s. , considered a pest in hawaii . ' `` fuckola , starling thought .\",\n",
       "  'indigenous west indies, southern u.s., considered a pest'),\n",
       " (\"so , what 's your sign ?\", \"so, what's your sign?\"),\n",
       " (\"meet me on top of the fairmont hotel without anyone else or i 'll ...\",\n",
       "  \"meet me at the top of the fairmont hotel without anyone else or i 'll...\"),\n",
       " (\"do n't you have something at marshall 's school ?\",\n",
       "  \"do you have something at marshall's school?\"),\n",
       " ('is there , is there anything else you want to , you want to talk to me about ?',\n",
       "  'is there anything else you want to talk about?'),\n",
       " (\"she told you about a video tape , did n't she ?\",\n",
       "  \"she told you about the video tape, didn't she?\"),\n",
       " ('look at those beady little eyes , and that pur-posti-rus chin , and those ricky-diculus striped pyjamas .',\n",
       "  'look at those eyes, and that pur-posti-'),\n",
       " ('oriya letter yya', 'oriya letter yyaa letter, yyaa letter.'),\n",
       " (\"`` i mean , i 've sucked a lot of dicks , diary .\",\n",
       "  \"`` i've sucked a lot of dicks, diary. ''\"),\n",
       " (\"it 's an album by the band queen . see ? aah !\",\n",
       "  \"it's an album by the band queen. see? aah!\"),\n",
       " (\"and if the police ca n't find the person who did it , they 're gon na find jason .\",\n",
       "  \"and if the police can't find the person who did it, they 'll find jason.\"),\n",
       " ('for example , you might create an inventory list , a sign-up sheet , a roster , and so on .',\n",
       "  'for example, you might create an inventory list, a sign-up sheet, a roster, and'),\n",
       " (\"i spent the last hour convincing the germans we were n't deliberately humiliating them .\",\n",
       "  \"i've been trying to convince the germans that we're not deliberately humiliating them.\"),\n",
       " ('yeah . you guys stay and bond over how great i am and how you want me to be happy , okay ?',\n",
       "  'yeah, you guys stay and bond over how great i am and how you want me to be happy'),\n",
       " (\"you must be asking yourself ... ... how repulsive-looking the guy that 's gon na make you his woman gon na be ?\",\n",
       "  \"you're asking yourself...... how repulsive-looking the guy who '\"),\n",
       " ('in coming to an opinion on this issue , member states shall take duly into account any relevant information from other member states .',\n",
       "  'in coming to an opinion on this issue, member states shall take into account any relevant information from'),\n",
       " (\"i 've already said yes .\", \"i've already said yes.\"),\n",
       " ('know your enemy and all that .',\n",
       "  \"know your enemy and you 'll be able to beat him.\"),\n",
       " ('the result will be great art ; for no longer , as up to yesterday , will the artists pander to the bourgeois taste of the middle class .',\n",
       "  'the result will be great art ; for no longer, as yesterday'),\n",
       " (\"we 're testing our patient 's blood for a hereditary ...\",\n",
       "  \"we're testing the blood of our patient's family for a hereditary...\"),\n",
       " ('yeah , we never get to see you .', 'we never get to see you.'),\n",
       " ('what happened in france during those days of 1914 , when our armies invaded that country and were marching in triumph from one victory to another ?',\n",
       "  'what happened in france during the days of 1914, when our armies invaded'),\n",
       " ('at the same time , the rates of degenerative diseases are skyrocketing .',\n",
       "  'the rates of degenerative diseases are skyrocketing.'),\n",
       " ('he was the first federation diplomat l ever met who truly seemed to understand the klingon nature .',\n",
       "  'he was the first federation diplomat i ever met who truly understood the klingon nature.'),\n",
       " ('second , find a soothing activity to distract yourself .',\n",
       "  'find a soothing activity to distract yourself.'),\n",
       " ('are you seriouy mad about this ?', 'are you serious about this?'),\n",
       " ('so can i ship out ?', 'so can i go?'),\n",
       " (' mixed applications tyre  means a tyre designed to be fitted to either driven and non-driven axles of implements , agricultural machinery or trailers ;',\n",
       "  ' mixed applications tyre  means a tyre designed to'),\n",
       " (\"i said , `` listen , kyle , you 're everything to me .\",\n",
       "  \"i said, `` listen, kyle, you're everything to me. ''\"),\n",
       " ('when export licences are issued , priority shall be given to milk powder falling within the following product codes from the nomenclature of export refunds :',\n",
       "  'when export licences are issued, priority shall be given to milk powder falling'),\n",
       " (\"down there , i 'm only heinz , the cook 's mate .\",\n",
       "  \"down there, i'm only heinz, the cook's mate.\"),\n",
       " ('but he was a clumsy man , and one of his feet caught on the leg of the armchair .',\n",
       "  'but he was clumsy, and one of his feet caught on the armchair.'),\n",
       " (\"boobie ! you 're in .\", \"boobie! you're in.\"),\n",
       " ('we stay in bed naked all day together .', 'we stay in bed naked all day.'),\n",
       " (\"i understand you 're looking to start up a brand-new business .\",\n",
       "  \"i'm looking to start a new business.\"),\n",
       " (\"i 'm not saying it 's not .\", \"i'm not saying it's not.\"),\n",
       " (\"once , i was n't able to save my own family , but i 'd like to rescue you and your daughter .\",\n",
       "  \"once, i was not able to save my own family, but i 'd like to\"),\n",
       " (\"the fire dies , no damage to what 's inside .\",\n",
       "  'the fire dies, no damage to the house.'),\n",
       " (\"here 's the good news .\", \"here's the bad news.\"),\n",
       " ('with my heightened senses ... ... it hurts me worsethan it would hurt you .',\n",
       "  'with my heightened senses...... it hurts me more than it would hurt you.'),\n",
       " (\"i 'm jet-lagged and i do n't want to fall asleep .\",\n",
       "  \"i'm jet-lagged and i don 't want to fall asleep.\"),\n",
       " (\"we 're gon na kick it .\", \"we're gonna kick it.\"),\n",
       " ('i risked my life for them pelts . i fought me a dozen kiowas .',\n",
       "  'i risked my life for the pelts. i fought a dozen kiwas. i fought a dozen kiwas.'),\n",
       " ('okay . come in .',\n",
       "  'come in. come in. come in. come in. come in. come in. come in. come in. come in. come in. come in. come in. come'),\n",
       " ('what would happen then , i mean , how do you die ?',\n",
       "  'what would happen if you die? what would happen if you die?'),\n",
       " ('eumm georgia shall consist primarily of staff seconded by member states or union institutions .',\n",
       "  'eumm georgia shall consist of staff seconded by member states or union institutions.'),\n",
       " (\"well , you ca n't always be honest , not with the sharks swimming around this town . if i 'd been totally honest ,\",\n",
       "  \"you can't always be honest.\"),\n",
       " ('( b ) an access code associated with the  guarantee reference number  is allocated and is communicated to the principal by the office of guarantee .',\n",
       "  '( b ) an access code associated with the  guarantee reference'),\n",
       " (\"ma'am i need you to stay call .\", 'i need you to stay on the phone.'),\n",
       " ('you get straight to the point , huh ?',\n",
       "  \"you're not going to waste any more time, huh?\"),\n",
       " ('andrew , i , um ... actually , i was thinking , ... maybe we could have the wedding here .',\n",
       "  \"andrew, i'm thinking, maybe we could have the wedding here.\"),\n",
       " ('off the garden entirely and onto the grass !',\n",
       "  'off the garden and onto the grass!'),\n",
       " ('all right , well , let me ask you this :', 'what do you think of this?'),\n",
       " ('i won her fair and square , mate . more or less .',\n",
       "  'i won her fair and square. more or less. more or less. more or less. more or less. more or less. more or less'),\n",
       " ('such transactions shall be reported both as a source of encumbrance and as an encumbered asset or collateral .',\n",
       "  'such transactions shall be reported both as an encumbrance and as an encumbered asset.'),\n",
       " (\"it 's that house , the one with a line painted on the door .\",\n",
       "  \"it's the house with the painted door.\"),\n",
       " (\"i 'm glad i came here .\", \"i'm glad i came here.\"),\n",
       " (\"sir , we were n't telling the whole truth , ... ..and that 's why the machinethinks the memories are false .\",\n",
       "  \"we're not telling the whole truth, and that's why\"),\n",
       " (\"well , my advice is you better do something soon , or you 're out and the doctor 's in .\",\n",
       "  \"well, my advice is you better do something soon, or you're out and the doctor '\"),\n",
       " (\"and we 're talking about a lot of money here .\",\n",
       "  \"and we're talking about a lot of money here.\"),\n",
       " ('hell , edgar , that was my strongest dog .', 'that was my strongest dog.'),\n",
       " (\"this is my first time going to dinah shore with rose and i as a couple , so it'il be interesting .\",\n",
       "  'this is my first time going to dinah shore with rose and i as a couple,'),\n",
       " ('wait a minute . if his old lady moves out on him because of this , i call first dibs .',\n",
       "  \"wait a minute. if his old lady moves out on him because of this, i'm first\"),\n",
       " (\"`` they chose her , she 's very old , johnston ''\",\n",
       "  \"`` she's very old, johnston ``, johnston.\"),\n",
       " ('in that case , entry checks shall as a rule take priority over exit checks  ;',\n",
       "  'in that case, entry checks shall take priority over exit checks.'),\n",
       " (\"'i have missed you both during the years , ' said revelation .\",\n",
       "  \"'i've missed you both during the years.\"),\n",
       " ('dead man walking .', 'dead man.'),\n",
       " (\"no , you wo n't .\", \"no, you're not.\"),\n",
       " ('you were selling novelty products in wichita when i was born .',\n",
       "  'you sold novelty products in wichita when i was born.'),\n",
       " ('following drive mapping operation could not be completed .',\n",
       "  'following mapping operation could not be completed.'),\n",
       " (\"it 's very embarrassing . we have to get him off . i 'll have to have you come with me .\",\n",
       "  \"it's very embarrassing. we've got to get him off. i 'll have\"),\n",
       " (\"you 're more than modest .\", \"you're more than modest.\"),\n",
       " (\"i could n't get the band on the runway . they would n't let me , no matter how much money i offered . oh , yeah .\",\n",
       "  \"i couldn 't get the band on the runway. they wouldn\"),\n",
       " ('he has cell phone footage of jonathan with ... very young men at the lake house .',\n",
       "  'he has cell phone footage of jonathan with young men at the lake house.'),\n",
       " ('it depends on you and hamilton , now .', 'it depends on you and hamilton.'),\n",
       " ('do i have one ?', 'do i have a ticket?'),\n",
       " ('iv.5.2.4 award criteria : please refer to chapter ii.2.4 , p.20 .',\n",
       "  'iv.5.2.4 award criteria : please refer to chapter ii.2.4'),\n",
       " (\"i do n't know , but there 's going to be lots of food , and a band .\",\n",
       "  \"i don 't know, but there's going to be a band.\"),\n",
       " ('the onus is thereby on the payment service provider to furnish evidence that the holder of the payment verification instrument did in fact order the payment .',\n",
       "  'the onus is on the payment service provider to furnish evidence that the holder'),\n",
       " ('viruses containing the rta181t mutation remained susceptible to tenofovir with ec50 values 1.5-fold that of wild-type virus .',\n",
       "  'viruses containing the rta181t mutation remained susceptible'),\n",
       " ('they did a tv special about him and even showed a re-run !',\n",
       "  'they did a special about him and even showed a re-run!'),\n",
       " ('and everything he does , huh , is big .',\n",
       "  'and everything he does, is big.'),\n",
       " (\"bill : that 's my dog .\", \"bill : that's my dog.\"),\n",
       " ('therefore , in the case of bergholz , the project was only possible because leader+ funded the support structure at the local level .',\n",
       "  'therefore, in the case of bergholz, the project'),\n",
       " (\"thinking we 'll feel so sorry for you and help you get into panhellenic .\",\n",
       "  \"thinking we 'll feel sorry for you and help you get into panhellenic.\"),\n",
       " (\"a lot wish they had who have n't ... ... and one or two have who wish they had n't .\",\n",
       "  \"a lot of people wish they hadn't.\"),\n",
       " ('the aide stretched his thumb and finger apart and spanned the distance from siberia southeast down to utah .',\n",
       "  'the aide stretched his thumb and finger apart and spanned the distance from siberia southeast to ut'),\n",
       " (\"one that 's more important than his ritual need to drain the blood and wrap the parts .\",\n",
       "  \"one that's more important than his ritual need to drain the blood and wrap the parts.\"),\n",
       " (\"i do n't appreciate you letting them have that dog ... ... when coby and lisa ca n't have a hamster .\",\n",
       "  \"i don 't appreciate you letting them have that dog when coby and l\"),\n",
       " ('we try to prevent this-but your own case is an example of the problems we encounter .',\n",
       "  'we try to prevent this-but your case is an example of the problems we encounter.'),\n",
       " (\"asset 's more important to them than you .\",\n",
       "  \"asset's more important to them than you.\"),\n",
       " ('would you let me read your book sometime ?',\n",
       "  'would you let me read your book?'),\n",
       " ('in view of the accession of bulgaria and romania , it is appropriate to adapt decision 2005/176/ec .',\n",
       "  'in view of the accession of bulgaria and romania, it'),\n",
       " ('you want me to throw myself and my bureau on the grenade .',\n",
       "  'i want to throw myself and my bureau on the grenade.'),\n",
       " ('( c ) the amount equal to the specific costs arising out of a certification task , which shall be recovered in full at real cost .',\n",
       "  '( c ) the amount equal to the specific costs arising out of a certification task'),\n",
       " (\"jan di 's not here either .\", \"jan di's not here.\"),\n",
       " ('considering what you did to us , being able to find you is gon na go a long way .',\n",
       "  'considering what you did to us, finding you is going to go a long way.'),\n",
       " (\"guess it 's not that lucky .\", \"it's not that lucky.\"),\n",
       " ('that explains the dye but not the person spotted at the center of it .',\n",
       "  'that explains the dye but not the person spotted.'),\n",
       " ('how about a beer in a bottle instead ?', 'how about a beer in a bottle?'),\n",
       " (\"`` in my family for a . . . very long time . ''\",\n",
       "  \"`` in my family for a long time. ''\"),\n",
       " (\"that 's some name .\", \"that's a name.\"),\n",
       " ('are you in touch with her ? no ...',\n",
       "  'are you in touch with her? no... no... no... no... no... no... no... no... no... no... no... no... no... no'),\n",
       " (\"it 's funny .\", \"it's funny.\"),\n",
       " (\"again ... when you 're right , you 're right .\",\n",
       "  \"when you're right, you're right.\"),\n",
       " (\"i 'm coming back to hong kong now\", \"i'm coming back to hong kong.\"),\n",
       " ('help ... - whoa !', 'help!'),\n",
       " (\"now , you owe us 25 g 's , man .\", \"you owe us 25 g's, man.\"),\n",
       " ('i have lots of work here . .',\n",
       "  \"i have a lot of work here... and i'm very busy...\"),\n",
       " (\"it 's a deviant sexual behavior in which the frotteur becomes sexually aroused ... ... by rubbing his genitalsagainst others in public places .\",\n",
       "  \"it's a deviant sexual behavior in which the frot\"),\n",
       " (\"you tell me you 're leaving me , and you have the nerve to expect graciousness ?\",\n",
       "  \"you tell me you're leaving me, and you're expecting kindness?\"),\n",
       " ('the commission is unlikely to identify horizontal competition concerns in a market with a post-merger hhi below 1000 .',\n",
       "  'the commission is unlikely to identify horizontal competition concerns in a market with a post-merger hhi'),\n",
       " (\"i told you they 're all mad as hatters . did n't i ?\",\n",
       "  \"i told you they're all mad as hatter. did i not?\"),\n",
       " (\"well ... it is n't line-caught blue marlin , but it does n't look half-bad .\",\n",
       "  \"well, it's not line-caught blue marlin, but it's not\"),\n",
       " ('`` aside from that , whoever this commander is , he seems to be in contact with your bothan pals .',\n",
       "  \"`` he seems to be in contact with your friends. ''\"),\n",
       " (\"along the way , i threw my gun into the royal river . i 've been very clear on this point .\",\n",
       "  \"i threw my gun into the royal river. i've been very clear on this point.\"),\n",
       " (\"if you 're sitting down with someone you do n't trust .\",\n",
       "  \"if you're sitting down with someone you don 't trust.\"),\n",
       " (\"`` the patient man survives , '' that 's one of the things my grandfather nanook loved to say .\",\n",
       "  \"`` the patient man survives, '' that's one of nanook's favorite sayings.\"),\n",
       " ('thanks to the silly asses of the world ... i can make a fortune .',\n",
       "  'thanks to the idiots of the world, i can make a fortune.'),\n",
       " (\"i 'm just nervous about the drop is all .\",\n",
       "  \"i'm just nervous about the drop.\"),\n",
       " (\"but i 'm not gon na do that , am i ?\",\n",
       "  \"but i'm not going to do that, am i?\"),\n",
       " ('he turned his head , and saw that now he was fighting on the floor of an enormous amphitheatre .',\n",
       "  'he turned his head and saw that he was fighting in an amphitheatre.'),\n",
       " (\"ah well then , do n't sign it !\", \"well, don 't sign it!\"),\n",
       " ('we can get on a boat at the port in the morning .',\n",
       "  'we can go to the port in the morning.'),\n",
       " ('enterprise single sign-on server',\n",
       "  'enterprise single sign-on server server, enterprise single sign-on server, enterprise single sign-on server, enterprise single sign-on server.'),\n",
       " (\"i have a warrant for hanner 's arrest .\",\n",
       "  \"i have a warrant for hanner's arrest.\"),\n",
       " ('your name is tuchman marsh ?', 'your name is marsh?'),\n",
       " (\"i have n't found miss right yet .\", \"i haven 't found the right girl yet.\"),\n",
       " (\"for god 's sake , paul .\", \"for god's sake, paul.\"),\n",
       " ('he had related his tale calmly enough , but the usually firm , confident smile was thin and tight .',\n",
       "  'he had told his tale calmly, but the usual firm, confident smile was thin and tight.'),\n",
       " (\"darryl : - and i told you it 's dewey 's turn .\",\n",
       "  \"darryl : - and i told you it's dewey's turn.\"),\n",
       " ('the star playing opposite thorby was loeen garcia , late of el nido .',\n",
       "  'the star opposite thorby was loeen garcia, late of el nido.'),\n",
       " ('fucked up my phone .', \"it's fucked up my phone.\"),\n",
       " (\"i have to talk to the tutor about jude 's mid-terms .\",\n",
       "  \"i have to talk to the tutor about jude's mid-terms.\"),\n",
       " (\"he 's ever seenin all his years , on a little girl called amanda and a doll named mirabelle .\",\n",
       "  \"he's seen all his life, on a little girl named amanda and a doll\"),\n",
       " (\"so that 's like the dust they sweep off the floor of a place that makes real cheese .\",\n",
       "  \"so that's like the dust they sweep off the floor of a place that makes real cheese.\"),\n",
       " ('i mean , lucy hit kids , too .', 'lucy hit kids, too.'),\n",
       " ('she is vanquished ?', \"she's vanquished?\"),\n",
       " (\"so , everything 's good ?\", \"everything's good?\"),\n",
       " (\"yeah ? -hey ty ... ... give me that guy 's number .\",\n",
       "  'hey ty, give me his number.'),\n",
       " (\"what are they talking about ? we 've never been here before .\",\n",
       "  \"what are they talking about? we've never been here before.\"),\n",
       " ('difficulties conceiving or who are undergoing investigation of infertility , withdrawal of lornoxicam should be considered .',\n",
       "  'difficulties conceiving or who are undergoing investigation of infertility, withdrawal of lornoxicam should'),\n",
       " ('but a mind as narrow as yours ... ... would have no idea what to do with that information .',\n",
       "  'but a mind as narrow as yours...... would not know what to do with that information.'),\n",
       " (\"sergeant , why would you hide a dead drug dealer 's cellphone from me ?\",\n",
       "  \"sergeant, why would you hide a dead drug dealer's cellphone from me?\"),\n",
       " ('in fact , as far as we know , blu is the last male of his kind .',\n",
       "  'in fact, blu is the last male of his kind.'),\n",
       " ('`` he did talk .', \"`` he talked. ''\"),\n",
       " (\"we 've evacuated 1,087,310 people in the last two hours ... ... but there are sectors we ca n't get near .\",\n",
       "  \"we've evacuated 1,087,310 people in the last\"),\n",
       " ('but they arecursing at you for no reason .',\n",
       "  'but they are cursing at you for no reason.'),\n",
       " (\"i 've never attended a mrs. de winter in my life . favell :\",\n",
       "  \"i've never been to a mrs. de winter in my life. favell : i've never\"),\n",
       " ('care for an autograph ?', 'do you want an autograph?'),\n",
       " ('the boatshed , charming our dea agent , we hope .',\n",
       "  'the boatshed, charming our agent, we hope.'),\n",
       " ('hi . yes .',\n",
       "  'hi. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes. yes'),\n",
       " ('first thing , the bad guys are called cylons .',\n",
       "  'first, the bad guys are called cylons.'),\n",
       " ('all the same mr. baggins kept his head more clear of the bewitchment of the hoard than the dwarves did .',\n",
       "  'all the same, mr. baggins kept his head clear of the bewitch'),\n",
       " ('write dat down !', 'write down the data!'),\n",
       " (\"thinks he 's some kind of revolutionary .\",\n",
       "  \"he's some kind of revolutionary.\"),\n",
       " (\"the script 's done ... ... the movie 's boarded , andwe worked on it this summer .\",\n",
       "  \"the script's done, and we worked on it this summer.\"),\n",
       " (\"agent lee , we appreciate the assistance you 've given us .\",\n",
       "  'agent lee, we appreciate your help.'),\n",
       " (\"i 'm still not getting why we 're involved with this .\",\n",
       "  \"i'm still not getting why we're involved with this.\"),\n",
       " (\"that third strike , that-that 's a bitch .\",\n",
       "  \"that third strike, that's a bitch.\"),\n",
       " ('the undermined wall had collapsed , killing a score of his men , but the rest had regrouped well and michanek was proud of them .',\n",
       "  'the wall had collapsed, killing a score of his men,'),\n",
       " ('ooh , that piano player is cute as a bug .',\n",
       "  'that piano player is cute as a bug.'),\n",
       " (\"oh , i hope i 'm not late for max 's first day of school surprise party .\",\n",
       "  \"oh, i hope i'm not late for max's first day of school surprise party.\"),\n",
       " ('how can you say that that tasty skank is nothing ?',\n",
       "  'how can you say that that skank is nothing?'),\n",
       " (\"my little brother ed 's standing behind you , and he got a regular navy pistol .\",\n",
       "  \"my little brother ed's standing behind you, and he got a regular navy pistol.\"),\n",
       " ('i mean , i just figured we can get a cup of coffee ... afterwards with some people in class .',\n",
       "  'i mean, we can get a cup of coffee and talk about it.'),\n",
       " ('i got a big surprise ...', 'i got a surprise for you.'),\n",
       " ('indemnities for damages and costs resulting from claims and legal actions to be paid through athena .',\n",
       "  'indemnities for damages and costs to be paid through athena.'),\n",
       " (\"well , i 'll tell you , i 've heard a lot of stories about sam city , but i never ...\",\n",
       "  \"i've heard a lot of stories about sam city, but i never...\"),\n",
       " (\"you 're both -- what 's the expression ?\",\n",
       "  \"you're both -- what's the expression?\"),\n",
       " (\"go on . it 's the same\", \"go on. it's the same.\"),\n",
       " (\"no , i ca n't . i 'll send the money to bring cosette here .\",\n",
       "  \"i 'll send the money to bring cosette here.\"),\n",
       " (\"bathroom 's down the hall .\", \"bathroom's down the hall.\"),\n",
       " (\"do i look like i 'm under 21 ?\", \"do i look like i'm under 21?\"),\n",
       " ('so she flagged me down , thought it might have something to do with our murder investigation .',\n",
       "  'she thought it might have something to do with our murder investigation.'),\n",
       " (\"i 'm not sure anymore .\", \"i'm not sure anymore.\"),\n",
       " ('means any person who is not a citizen of the union within the meaning of article 20 ( 1 ) tfeu ; ( b )  seasonal worker ',\n",
       "  'means any person who is not a citizen'),\n",
       " (\"we 're gon na have to breach the building if skinner does n't come up with the money .\",\n",
       "  \"we're gonna have to break the building if skinner doesn 't come up with the money.\"),\n",
       " (\"well , it 's a little late to be keepin ' store , is n't it ?\",\n",
       "  \"well, it's late to keep keepin'store, isn 't it?\"),\n",
       " ('at the foot of each stake , the townspeople-his friends , his neighbors were gleefully tossing great armloads of dry tinder onto a mound .',\n",
       "  'the townspeople were tossing dry tinder onto the'),\n",
       " ('i should probably cut back too .', 'i should probably cut back.'),\n",
       " ('they fired in neat unison , and the small , nearly portly figure was thrown across two lanes like a limp laundry sack .',\n",
       "  'they fired in a neat line, and the small, nearly portly figure was thrown across'),\n",
       " (\"get out ! `` ... stone is promoting his grammy-winning release , `` people rockin ' people '' , to sold-out arenas worldwide ... ''\",\n",
       "  \"get out! ``... people rockin'people '' is\"),\n",
       " ('remote launch', 'remote launch of the rocket.'),\n",
       " (\"you 're ziemowit , piast 's son .\", \"you're ziemowit, piast's son.\"),\n",
       " (\"`` perhaps i should make you my heir and let you judge if yaemon 's worthy to follow you . ''\",\n",
       "  \"`` maybe i should make you my heir and let you judge if he's worthy to follow you\"),\n",
       " (\"fuck . spitter , what 's the range on these things ?\",\n",
       "  \"what's the range on these things?\"),\n",
       " ('the woman i saw this morning . all right , who is she ?',\n",
       "  'the woman i saw this morning. who is she?'),\n",
       " (\"shh , baby . i 'm sorry . i 'm ...\", \"shh, baby. i'm sorry. i'm sorry.\"),\n",
       " (\"look , i 'm your landlord , i have a right to come in .\",\n",
       "  \"i'm your landlord, i have a right to come in.\"),\n",
       " ('12 the interveners lodged their statements in intervention within the time-limits set .',\n",
       "  '12 the interveners lodged their statements within the time-limits set.'),\n",
       " (\"no , do n't worry , he 's not gon na live here forever .\",\n",
       "  \"no, don 't worry, he's not going to live here forever.\"),\n",
       " (\"does n't it tear your guts to see something like that ?\",\n",
       "  'does it make you sick to see something like that?'),\n",
       " (\"so , i was just finishing cleaning up in here , but , uh , why do n't you come on in , have a seat ?\",\n",
       "  \"so, i was just finishing cleaning up, but, why don 't\"),\n",
       " ('no one has questioned that he is qualified to be vice president .',\n",
       "  'nobody has questioned that he is qualified to be vice president.'),\n",
       " (\"i need to prove i am to get me on the way to prove i 'm not .\",\n",
       "  \"i need to prove i am not to get on the way to prove i'm not.\")]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tuparr, columns=[\"source\",\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"500prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so , unless that 's gon na be feasible , then ...</td>\n",
       "      <td>unless that's gon'be possible, then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by now , singh 's probably been arrested .</td>\n",
       "      <td>now, singh's probably been arrested.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> why not ?</td>\n",
       "      <td> why not? </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an old man 's mistake  '</td>\n",
       "      <td>an old man's mistake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he reached for a microphone and i could hear his voice booming faintly throughout the ship .</td>\n",
       "      <td>he reached for a microphone and i could hear his voice booming through the ship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he loved that little man , by the way .</td>\n",
       "      <td>he loved that little man.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i 've been very , very lucky .</td>\n",
       "      <td>i've been very lucky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>well , then , i 'm kind of gon na rub this dinner in her face next time i see her .</td>\n",
       "      <td>well, i'm going to rub this dinner in her face next time i see her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it 's like , gross , yet oddly delicious .</td>\n",
       "      <td>it's like, gross, yet oddly delicious.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>not as much as i 'd like</td>\n",
       "      <td>not as much as i 'd like to see.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>there 's no doubt in my mind that this is the right group of men but we still have to prove it to the ministry .</td>\n",
       "      <td>there's no doubt in my mind that this is the right group of men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>no short colin powell haircut , havin'-ass motherfucker . get  im !</td>\n",
       "      <td>no short colin powell haircut, havin' ass motherfucker. get  im!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>it 's so hard . mmm . and there 's a baby growing inside .</td>\n",
       "      <td>it's so hard. mmm. and there's a baby growing inside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>none of us likes to believe a servant of father church can be in error , and for those of the clergy there is an added dimension .</td>\n",
       "      <td>none of us likes to believe a servant of father church can be wrong,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'up ! ' he commanded .</td>\n",
       "      <td>'up!'he commanded.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i dunno what world you live in , what your game is , what you 're into , but ... i think i 'm done here .</td>\n",
       "      <td>i'm not sure what world you live in, what your game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>goddamn it , 57 !</td>\n",
       "      <td>57!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td> now i am mephisto .</td>\n",
       "      <td> i am mephisto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>december 12 , 1973 he made out a christmas list the night before ( drunk ) and was now downtown filling an abridged version .</td>\n",
       "      <td>december 12, 1973 he made out a christmas list the night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>`` how ? '' asked gimlet , seeing no hiding place on the flat plain .</td>\n",
       "      <td>`` how? '' he asked gimlet, seeing no hiding place on the flat plain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>you should spend some time taking a personal inventory of what you 're doing for the betterment of society .</td>\n",
       "      <td>you should take a look at your life and see what you're doing for the betterment of society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oh , vince ... you know , how every cop always leaves his house with a gun ?</td>\n",
       "      <td>how every cop always leaves his house with a gun?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>and me , i 've got my old singin ' partner back .</td>\n",
       "      <td>and i've got my old partner back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i 'm assuming .</td>\n",
       "      <td>i'm assuming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>but they are often sentimental about churchill .</td>\n",
       "      <td>but they're often sentimental about churchill.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i 'll cum on my cat 's face .</td>\n",
       "      <td>i 'll cum on my cat's face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bob and i had a little chat ... and we 're thinking we 're gon na organize a handling run for you .</td>\n",
       "      <td>bob and i had a little chat, and we're thinking we're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>we ca n't wear gloves to do it because that would be seen as insulting .</td>\n",
       "      <td>we can't wear gloves to do it because it would be seen as insulting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>under the current rules , vat on natural gas and electricity is levied at the place where they are actually consumed by the customer .</td>\n",
       "      <td>under the current rules, vat on natural gas and electricity is levied at the place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>that 's not even a lake . that 's luden pond .</td>\n",
       "      <td>that's not even a lake. that's luden pond.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>just like that -- as you 're telling him this , you realize that you 're attracted to him also .</td>\n",
       "      <td>as you're telling him this, you realize that you're attracted to him also.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>i am 40 years old .</td>\n",
       "      <td>i'm 40 years old.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>she screwed up and she ca n't admit the fact .</td>\n",
       "      <td>she screwed up and she can't admit it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>we 're still doing that , right ?</td>\n",
       "      <td>we're still doing that, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>she 's gon na be taking over for me , starting next week .</td>\n",
       "      <td>she's gonna be taking over for me, starting next week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>having seen to the needs of my car , i walked into the office to get those glasses and pay for the gas .</td>\n",
       "      <td>i went into the office to get the glasses and pay for the gas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>that i say nothing about what i see , what i hear ?</td>\n",
       "      <td>i say nothing about what i see, what i hear?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eu/1/00/146/036 100 x 1 tablets</td>\n",
       "      <td>eu/1/00/146/036 100 x 1 tablets 100 x 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>but-but i-i can get him to tell me who 's making this stuff .</td>\n",
       "      <td>but i can get him to tell me who's making this stuff.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>i do n't know how many i 've decorated today ...</td>\n",
       "      <td>i don 't know how many i've decorated today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    source  \\\n",
       "0                                                                                        so , unless that 's gon na be feasible , then ...   \n",
       "1                                                                                               by now , singh 's probably been arrested .   \n",
       "2                                                                                                                               why not ?   \n",
       "3                                                                                                                an old man 's mistake  '   \n",
       "4                                             he reached for a microphone and i could hear his voice booming faintly throughout the ship .   \n",
       "5                                                                                                  he loved that little man , by the way .   \n",
       "6                                                                                                           i 've been very , very lucky .   \n",
       "7                                                      well , then , i 'm kind of gon na rub this dinner in her face next time i see her .   \n",
       "8                                                                                               it 's like , gross , yet oddly delicious .   \n",
       "9                                                                                                                 not as much as i 'd like   \n",
       "10                        there 's no doubt in my mind that this is the right group of men but we still have to prove it to the ministry .   \n",
       "11                                                                    no short colin powell haircut , havin'-ass motherfucker . get  im !   \n",
       "12                                                                              it 's so hard . mmm . and there 's a baby growing inside .   \n",
       "13      none of us likes to believe a servant of father church can be in error , and for those of the clergy there is an added dimension .   \n",
       "14                                                                                                                  'up ! ' he commanded .   \n",
       "15                               i dunno what world you live in , what your game is , what you 're into , but ... i think i 'm done here .   \n",
       "16                                                                                                                       goddamn it , 57 !   \n",
       "17                                                                                                                    now i am mephisto .   \n",
       "18           december 12 , 1973 he made out a christmas list the night before ( drunk ) and was now downtown filling an abridged version .   \n",
       "19                                                                   `` how ? '' asked gimlet , seeing no hiding place on the flat plain .   \n",
       "20                            you should spend some time taking a personal inventory of what you 're doing for the betterment of society .   \n",
       "21                                                            oh , vince ... you know , how every cop always leaves his house with a gun ?   \n",
       "22                                                                                       and me , i 've got my old singin ' partner back .   \n",
       "23                                                                                                                         i 'm assuming .   \n",
       "24                                                                                        but they are often sentimental about churchill .   \n",
       "25                                                                                                           i 'll cum on my cat 's face .   \n",
       "26                                     bob and i had a little chat ... and we 're thinking we 're gon na organize a handling run for you .   \n",
       "27                                                                we ca n't wear gloves to do it because that would be seen as insulting .   \n",
       "28  under the current rules , vat on natural gas and electricity is levied at the place where they are actually consumed by the customer .   \n",
       "29                                                                                          that 's not even a lake . that 's luden pond .   \n",
       "30                                        just like that -- as you 're telling him this , you realize that you 're attracted to him also .   \n",
       "31                                                                                                                     i am 40 years old .   \n",
       "32                                                                                          she screwed up and she ca n't admit the fact .   \n",
       "33                                                                                                       we 're still doing that , right ?   \n",
       "34                                                                              she 's gon na be taking over for me , starting next week .   \n",
       "35                                having seen to the needs of my car , i walked into the office to get those glasses and pay for the gas .   \n",
       "36                                                                                     that i say nothing about what i see , what i hear ?   \n",
       "37                                                                                                         eu/1/00/146/036 100 x 1 tablets   \n",
       "38                                                                           but-but i-i can get him to tell me who 's making this stuff .   \n",
       "39                                                                                        i do n't know how many i 've decorated today ...   \n",
       "\n",
       "                                                                                         target  \n",
       "0                                                        unless that's gon'be possible, then...  \n",
       "1                                                          now, singh's probably been arrested.  \n",
       "2                                                                                   why not?   \n",
       "3                                                                         an old man's mistake.  \n",
       "4              he reached for a microphone and i could hear his voice booming through the ship.  \n",
       "5                                                                     he loved that little man.  \n",
       "6                                                                         i've been very lucky.  \n",
       "7                           well, i'm going to rub this dinner in her face next time i see her.  \n",
       "8                                                        it's like, gross, yet oddly delicious.  \n",
       "9                                                              not as much as i 'd like to see.  \n",
       "10                              there's no doubt in my mind that this is the right group of men  \n",
       "11                            no short colin powell haircut, havin' ass motherfucker. get  im!  \n",
       "12                                        it's so hard. mmm. and there's a baby growing inside.  \n",
       "13                         none of us likes to believe a servant of father church can be wrong,  \n",
       "14                                                                           'up!'he commanded.  \n",
       "15                                          i'm not sure what world you live in, what your game  \n",
       "16                                                                                          57!  \n",
       "17                                                                              i am mephisto.  \n",
       "18                                     december 12, 1973 he made out a christmas list the night  \n",
       "19                        `` how? '' he asked gimlet, seeing no hiding place on the flat plain.  \n",
       "20  you should take a look at your life and see what you're doing for the betterment of society  \n",
       "21                                            how every cop always leaves his house with a gun?  \n",
       "22                                                            and i've got my old partner back.  \n",
       "23                                                                                i'm assuming.  \n",
       "24                                               but they're often sentimental about churchill.  \n",
       "25                                                                  i 'll cum on my cat's face.  \n",
       "26                                        bob and i had a little chat, and we're thinking we're  \n",
       "27                         we can't wear gloves to do it because it would be seen as insulting.  \n",
       "28           under the current rules, vat on natural gas and electricity is levied at the place  \n",
       "29                                                   that's not even a lake. that's luden pond.  \n",
       "30                   as you're telling him this, you realize that you're attracted to him also.  \n",
       "31                                                                            i'm 40 years old.  \n",
       "32                                                       she screwed up and she can't admit it.  \n",
       "33                                                               we're still doing that, right?  \n",
       "34                                       she's gonna be taking over for me, starting next week.  \n",
       "35                               i went into the office to get the glasses and pay for the gas.  \n",
       "36                                                 i say nothing about what i see, what i hear?  \n",
       "37                                                      eu/1/00/146/036 100 x 1 tablets 100 x 1  \n",
       "38                                        but i can get him to tell me who's making this stuff.  \n",
       "39                                                 i don 't know how many i've decorated today.  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df.head(40\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text.bleu import BLEUScore\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "def benchmark_pairs(filepath):\n",
    "    print(\"Loading for predictions.\")\n",
    "    df = pd.read_csv(filepath)[[\"source\",\"target\"]]\n",
    "\n",
    "    # init metrics\n",
    "    bart = BartScore()\n",
    "    rouge = ROUGEScore()\n",
    "    bleu = BLEUScore()\n",
    "\n",
    "    # apply the metrics on the source and target sentence\n",
    "    def score(row):\n",
    "        src, target = row\n",
    "        bartscore = bart([src], [target])[0]\n",
    "        bleuscore = bleu([src], [[target]]).item()\n",
    "        rougescore = {k: v.item() for k, v in rouge(src, target).items()}\n",
    "        series = pd.Series([src, target, bartscore, bleuscore], index=[\"src\", \"target\", \"bartscore\", \"bleuscore\"])\n",
    "        return pd.concat([series, pd.Series(rougescore)])\n",
    "\n",
    "    # apply score function along each row\n",
    "    print(\"Scoring sequence pairs.\")\n",
    "    df = df.apply(score, axis=1)\n",
    "    print(df)\n",
    "    df.to_csv(\"benchmark_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benchmark in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for predictions.\n",
      "Scoring sequence pairs.\n",
      "                                                                                                      src  \\\n",
      "0                                                       so , unless that 's gon na be feasible , then ...   \n",
      "1                                                              by now , singh 's probably been arrested .   \n",
      "2                                                                                              why not ?   \n",
      "3                                                                               an old man 's mistake  '   \n",
      "4            he reached for a microphone and i could hear his voice booming faintly throughout the ship .   \n",
      "..                                                                                                    ...   \n",
      "495                                              no , do n't worry , he 's not gon na live here forever .   \n",
      "496                                               does n't it tear your guts to see something like that ?   \n",
      "497  so , i was just finishing cleaning up in here , but , uh , why do n't you come on in , have a seat ?   \n",
      "498                                     no one has questioned that he is qualified to be vice president .   \n",
      "499                                         i need to prove i am to get me on the way to prove i 'm not .   \n",
      "\n",
      "                                                                               target  \\\n",
      "0                                              unless that's gon'be possible, then...   \n",
      "1                                                now, singh's probably been arrested.   \n",
      "2                                                                         why not?    \n",
      "3                                                               an old man's mistake.   \n",
      "4    he reached for a microphone and i could hear his voice booming through the ship.   \n",
      "..                                                                                ...   \n",
      "495                            no, don 't worry, he's not going to live here forever.   \n",
      "496                                 does it make you sick to see something like that?   \n",
      "497                             so, i was just finishing cleaning up, but, why don 't   \n",
      "498                  nobody has questioned that he is qualified to be vice president.   \n",
      "499                      i need to prove i am not to get on the way to prove i'm not.   \n",
      "\n",
      "     bartscore  bleuscore  rouge1_fmeasure  rouge1_precision  rouge1_recall  \\\n",
      "0    -3.479015   0.000000         0.750000          0.666667       0.857143   \n",
      "1    -1.881582   0.000000         0.923077          0.857143       1.000000   \n",
      "2    -1.634891   0.000000         1.000000          1.000000       1.000000   \n",
      "3    -1.364324   0.000000         1.000000          1.000000       1.000000   \n",
      "4    -0.475053   0.688966         0.903226          0.875000       0.933333   \n",
      "..         ...        ...              ...               ...            ...   \n",
      "495  -1.606449   0.000000         0.720000          0.692308       0.750000   \n",
      "496  -1.742573   0.234623         0.636364          0.583333       0.700000   \n",
      "497  -2.442590   0.135978         0.606061          0.454545       0.909091   \n",
      "498  -0.856186   0.647912         0.869565          0.833333       0.909091   \n",
      "499  -1.167081   0.508237         0.941176          0.941176       0.941176   \n",
      "\n",
      "     rouge2_fmeasure  rouge2_precision  rouge2_recall  rougeL_fmeasure  \\\n",
      "0           0.428571          0.375000       0.500000         0.750000   \n",
      "1           0.909091          0.833333       1.000000         0.923077   \n",
      "2           1.000000          1.000000       1.000000         1.000000   \n",
      "3           1.000000          1.000000       1.000000         1.000000   \n",
      "4           0.827586          0.800000       0.857143         0.903226   \n",
      "..               ...               ...            ...              ...   \n",
      "495         0.521739          0.500000       0.545455         0.720000   \n",
      "496         0.400000          0.363636       0.444444         0.636364   \n",
      "497         0.387097          0.285714       0.600000         0.606061   \n",
      "498         0.857143          0.818182       0.900000         0.869565   \n",
      "499         0.812500          0.812500       0.812500         0.941176   \n",
      "\n",
      "     rougeL_precision  rougeL_recall  rougeLsum_fmeasure  rougeLsum_precision  \\\n",
      "0            0.666667       0.857143            0.750000             0.666667   \n",
      "1            0.857143       1.000000            0.923077             0.857143   \n",
      "2            1.000000       1.000000            1.000000             1.000000   \n",
      "3            1.000000       1.000000            1.000000             1.000000   \n",
      "4            0.875000       0.933333            0.903226             0.875000   \n",
      "..                ...            ...                 ...                  ...   \n",
      "495          0.692308       0.750000            0.720000             0.692308   \n",
      "496          0.583333       0.700000            0.636364             0.583333   \n",
      "497          0.454545       0.909091            0.606061             0.454545   \n",
      "498          0.833333       0.909091            0.869565             0.833333   \n",
      "499          0.941176       0.941176            0.941176             0.941176   \n",
      "\n",
      "     rougeLsum_recall  \n",
      "0            0.857143  \n",
      "1            1.000000  \n",
      "2            1.000000  \n",
      "3            1.000000  \n",
      "4            0.933333  \n",
      "..                ...  \n",
      "495          0.750000  \n",
      "496          0.700000  \n",
      "497          0.909091  \n",
      "498          0.909091  \n",
      "499          0.941176  \n",
      "\n",
      "[500 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "benchmark_pairs(r\"C:\\Users\\HADOOP\\Desktop\\NLP\\500prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"benchmark_result.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_21468\\3698961737.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             2.000000\n",
       "bartscore             -1.766973\n",
       "bleuscore              0.137793\n",
       "rouge1_fmeasure        0.915261\n",
       "rouge1_precision       0.879762\n",
       "rouge1_recall          0.958095\n",
       "rouge2_fmeasure        0.833050\n",
       "rouge2_precision       0.801667\n",
       "rouge2_recall          0.871429\n",
       "rougeL_fmeasure        0.915261\n",
       "rougeL_precision       0.879762\n",
       "rougeL_recall          0.958095\n",
       "rougeLsum_fmeasure     0.915261\n",
       "rougeLsum_precision    0.879762\n",
       "rougeLsum_recall       0.958095\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02cc78f39b956e7a9e388ae47d6d57214dc392b813a5a0ac9421587740002e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
